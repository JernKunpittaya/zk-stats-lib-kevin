{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../bench.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data00_path = os.path.join('data00.json')\n",
    "data01_path = os.path.join('data01.json')\n",
    "data02_path = os.path.join('data02.json')\n",
    "data10_path = os.path.join('data10.json')\n",
    "data11_path = os.path.join('data11.json')\n",
    "data12_path = os.path.join('data12.json')\n",
    "data20_path = os.path.join('data20.json')\n",
    "data21_path = os.path.join('data21.json')\n",
    "data22_path = os.path.join('data22.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_param_func(data_tensor_array):\n",
    "    # for 1 value, just have 1 element in tensor array\n",
    "    data_tensor = data_tensor_array[0]\n",
    "    return [torch.exp(torch.mean(torch.log(data_tensor)))]\n",
    "\n",
    "def model_func(param):\n",
    "    class verifier_model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(verifier_model, self).__init__()\n",
    "            self.w = nn.Parameter(data = param[0], requires_grad = False)\n",
    "\n",
    "        def forward(self,X):\n",
    "            # here is witness approach\n",
    "            # here for large data (100-10k), to satisfy 1% requires too big circuit, \n",
    "            # so here we use 5% deviation aka 1.05 instead of 1.01\n",
    "            return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.2)), self.w)\n",
    "    return verifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================  Data00: 50 small values  =====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_73470/3995111971.py:14: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.01)), self.w)\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":8,\"param_scale\":8,\"scale_rebase_multiplier\":10,\"lookup_range\":[-6,492],\"logrows\":12,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":3936,\"total_assignments\":160,\"total_const_size\":4,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,8],\"model_input_scales\":[8],\"module_sizes\":{\"kzg\":[],\"poseidon\":[3936,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":256.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.4.2\",\"num_blinding_factors\":null}\n",
      "gen prf time:  0.9415709972381592\n",
      "Theory result:  tensor(0.3472, dtype=torch.float64)\n",
      "Our result:  [0.34765625]\n"
     ]
    }
   ],
   "source": [
    "bench_one([data00_path], model_func,gen_param_func, \"Data00: 50 small values\",[8], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================  Data01: 50 medium values  =====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_73470/3995111971.py:14: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.01)), self.w)\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":8,\"param_scale\":8,\"scale_rebase_multiplier\":10,\"lookup_range\":[0,51148],\"logrows\":16,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":3936,\"total_assignments\":160,\"total_const_size\":4,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,8],\"model_input_scales\":[8],\"module_sizes\":{\"kzg\":[],\"poseidon\":[3936,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":256.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.4.2\",\"num_blinding_factors\":null}\n",
      "gen prf time:  9.68405294418335\n",
      "Theory result:  tensor(29.6977, dtype=torch.float64)\n",
      "Our result:  [29.69921875]\n"
     ]
    }
   ],
   "source": [
    "bench_one([data01_path], model_func,gen_param_func, \"Data01: 50 medium values\",[8], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================  Data02: 50 large values  =====================================\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":4,\"param_scale\":4,\"scale_rebase_multiplier\":10,\"lookup_range\":[-2,319168],\"logrows\":19,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":3936,\"total_assignments\":160,\"total_const_size\":4,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,4],\"model_input_scales\":[4],\"module_sizes\":{\"kzg\":[],\"poseidon\":[3936,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":16.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.4.2\",\"num_blinding_factors\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_74222/745357021.py:15: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.05)), self.w)\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen prf time:  70.78155088424683\n",
      "Theory result:  tensor(4406.3075, dtype=torch.float64)\n",
      "Our result:  [4406.3125]\n"
     ]
    }
   ],
   "source": [
    "bench_one([data02_path], model_func,gen_param_func, \"Data02: 50 large values\",[4], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================  Data10: 300 small values  =====================================\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":8,\"param_scale\":8,\"scale_rebase_multiplier\":10,\"lookup_range\":[-14,512],\"logrows\":14,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":14432,\"total_assignments\":910,\"total_const_size\":4,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,8],\"model_input_scales\":[8],\"module_sizes\":{\"kzg\":[],\"poseidon\":[14432,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":256.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.4.2\",\"num_blinding_factors\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_74222/745357021.py:15: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.05)), self.w)\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen prf time:  2.932950973510742\n",
      "Theory result:  tensor(0.3873, dtype=torch.float64)\n",
      "Our result:  [0.38671875]\n"
     ]
    }
   ],
   "source": [
    "bench_one([data10_path], model_func,gen_param_func, \"Data10: 300 small values\",[8], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================  Data11: 300 medium values  =====================================\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":7,\"param_scale\":7,\"scale_rebase_multiplier\":10,\"lookup_range\":[-2,25114],\"logrows\":15,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":14432,\"total_assignments\":910,\"total_const_size\":4,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,7],\"model_input_scales\":[7],\"module_sizes\":{\"kzg\":[],\"poseidon\":[14432,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":128.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.4.2\",\"num_blinding_factors\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_74222/745357021.py:15: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.05)), self.w)\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen prf time:  4.970611810684204\n",
      "Theory result:  tensor(39.5938, dtype=torch.float64)\n",
      "Our result:  [39.59375]\n"
     ]
    }
   ],
   "source": [
    "bench_one([data11_path], model_func,gen_param_func, \"Data11: 300 medium values\",[7], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================  Data12: 300 large values  =====================================\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":4,\"param_scale\":4,\"scale_rebase_multiplier\":10,\"lookup_range\":[0,319552],\"logrows\":19,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":14432,\"total_assignments\":910,\"total_const_size\":4,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,4],\"model_input_scales\":[4],\"module_sizes\":{\"kzg\":[],\"poseidon\":[14432,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":16.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.4.2\",\"num_blinding_factors\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_74222/745357021.py:15: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.05)), self.w)\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen prf time:  69.88444805145264\n",
      "Theory result:  tensor(3868.2301, dtype=torch.float64)\n",
      "Our result:  [3868.25]\n"
     ]
    }
   ],
   "source": [
    "bench_one([data12_path], model_func,gen_param_func, \"Data12: 300 large values\",[4], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================  Data20: 1000 small values  =====================================\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":15,\"param_scale\":15,\"scale_rebase_multiplier\":10,\"lookup_range\":[-3062,65536],\"logrows\":17,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":45920,\"total_assignments\":3010,\"total_const_size\":4,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,15],\"model_input_scales\":[15],\"module_sizes\":{\"kzg\":[],\"poseidon\":[45920,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":32768.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.4.2\",\"num_blinding_factors\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_74222/745357021.py:15: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.05)), self.w)\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen prf time:  18.7395658493042\n",
      "Theory result:  tensor(0.4015, dtype=torch.float64)\n",
      "Our result:  [0.4014892578125]\n"
     ]
    }
   ],
   "source": [
    "bench_one([data20_path], model_func,gen_param_func, \"Data20: 1000 small values\",[15], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================  Data21: 1000 medium values  =====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_74867/3849003573.py:15: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.05)), self.w)\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":11,\"param_scale\":11,\"scale_rebase_multiplier\":10,\"lookup_range\":[-200,409600],\"logrows\":19,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":45920,\"total_assignments\":3010,\"total_const_size\":4,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,11],\"model_input_scales\":[11],\"module_sizes\":{\"kzg\":[],\"poseidon\":[45920,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":2048.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.4.2\",\"num_blinding_factors\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen prf time:  70.13123178482056\n",
      "Theory result:  tensor(38.4133, dtype=torch.float64)\n",
      "Our result:  [38.4130859375]\n"
     ]
    }
   ],
   "source": [
    "bench_one([data21_path], model_func,gen_param_func, \"Data21: 1000 medium values\",[11], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================  Data22: 1000 large values  =====================================\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":6,\"param_scale\":6,\"scale_rebase_multiplier\":10,\"lookup_range\":[-12,1279104],\"logrows\":21,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":45920,\"total_assignments\":3010,\"total_const_size\":4,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,6],\"model_input_scales\":[6],\"module_sizes\":{\"kzg\":[],\"poseidon\":[45920,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":64.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.4.2\",\"num_blinding_factors\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_74867/459106111.py:17: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<=torch.log(torch.tensor(1.1)), self.w)\n",
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# very big, need to make very lenient accepted deviation like 20 or even 30% instead of merely 5 %\n",
    "bench_one([data22_path], model_func,gen_param_func, \"Data22: 1000 large values\",[6], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
