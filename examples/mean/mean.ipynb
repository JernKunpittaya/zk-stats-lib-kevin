{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==5.0.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 1)) (5.0.8)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 4)) (1.11.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 5)) (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: statistics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 7)) (1.0.3.5)\n",
      "Requirement already satisfied: onnx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (4.45.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from statistics->-r ../../requirements.txt (line 7)) (0.20.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnx->-r ../../requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->-r ../../requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->-r ../../requirements.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "def export_onnx(model, data_tensor_array, model_loc):\n",
    "  circuit = model()\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  # print(device)\n",
    "\n",
    "  circuit.to(device)\n",
    "\n",
    "  # Flips the neural net into inference mode\n",
    "  circuit.eval()\n",
    "  input_names = []\n",
    "  dynamic_axes = {}\n",
    "\n",
    "  data_tensor_tuple = ()\n",
    "  for i in range(len(data_tensor_array)):\n",
    "    data_tensor_tuple += (data_tensor_array[i],)\n",
    "    input_index = \"input\"+str(i+1)\n",
    "    input_names.append(input_index)\n",
    "    dynamic_axes[input_index] = {0 : 'batch_size'}\n",
    "  dynamic_axes[\"output\"] = {0 : 'batch_size'}\n",
    "\n",
    "  # Export the model\n",
    "  torch.onnx.export(circuit,               # model being run\n",
    "                      data_tensor_tuple,                   # model input (or a tuple for multiple inputs)\n",
    "                      model_loc,            # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=11,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = input_names,   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode is either \"accuracy\" or \"resources\"\n",
    "\n",
    "def gen_settings(comb_data_path, onnx_filename, scale, mode, settings_filename):\n",
    "  print(\"==== Generate & Calibrate Setting ====\")\n",
    "  # Set input to be Poseidon Hash, and param of computation graph to be public\n",
    "  # Poseidon is not homomorphic additive, maybe consider Pedersens or Dory commitment.\n",
    "  gip_run_args = ezkl.PyRunArgs()\n",
    "  gip_run_args.input_visibility = \"hashed\"  # matrix and generalized inverse commitments\n",
    "  gip_run_args.output_visibility = \"public\"   # no parameters used\n",
    "  gip_run_args.param_visibility = \"private\" # should be Tensor(True)--> to enforce arbitrary data in w\n",
    "\n",
    " # generate settings\n",
    "  ezkl.gen_settings(onnx_filename, settings_filename, py_run_args=gip_run_args)\n",
    "  if scale ==\"default\":\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode)\n",
    "  else:\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode, scales = scale)\n",
    "\n",
    "  assert os.path.exists(settings_filename)\n",
    "  assert os.path.exists(comb_data_path)\n",
    "  assert os.path.exists(onnx_filename)\n",
    "  f_setting = open(settings_filename, \"r\")\n",
    "  print(\"scale: \", scale)\n",
    "  print(\"setting: \", f_setting.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_define_calculation(verifier_model, verifier_model_path, dummy_data_path_array):\n",
    "  # load data from dummy_data_path_array into dummy_data_array\n",
    "  dummy_data_tensor_array = []\n",
    "  # comb_dummy_data = []\n",
    "  for path in dummy_data_path_array:\n",
    "    dummy_data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    dummy_data_tensor_array.append(torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 )))\n",
    "    # comb_dummy_data.append(dummy_data.tolist())\n",
    "  # export onnx file\n",
    "  export_onnx(verifier_model,dummy_data_tensor_array, verifier_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we decide to not have comb_data_path as parameter since a bit redundant parameter.\n",
    "def prover_gen_settings(data_path_array, verifier_model_path, scale, mode, settings_path):\n",
    "    comb_data = []\n",
    "    for path in data_path_array:\n",
    "        data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "        comb_data.append(data.tolist())\n",
    "    # this is private to prover since it contains actual data\n",
    "    comb_data_path = os.path.join('prover/comb_data.json')\n",
    "    # Serialize data into file:\n",
    "    json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "    # gen + calibrate setting\n",
    "    gen_settings(comb_data_path, verifier_model_path, scale, mode, settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here prover can concurrently call this since all params are public to get pk. \n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "def verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path ):\n",
    "  # compile circuit\n",
    "  res = ezkl.compile_circuit(verifier_model_path, verifier_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "\n",
    "  # srs path\n",
    "  res = ezkl.get_srs(srs_path, settings_path)\n",
    "\n",
    "  # setupt vk, pk param for use..... prover can use same pk or can init their own!\n",
    "  print(\"==== setting up ezkl ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.setup(\n",
    "        verifier_compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        srs_path)\n",
    "  end_time = time.time()\n",
    "  time_setup = end_time -start_time\n",
    "  print(f\"Time setup: {time_setup} seconds\")\n",
    "\n",
    "  assert res == True\n",
    "  assert os.path.isfile(vk_path)\n",
    "  assert os.path.isfile(pk_path)\n",
    "  assert os.path.isfile(settings_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prover_gen_proof(prover_model, data_path_array, witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path, pk_path, srs_path):\n",
    "  data_tensor_array = []\n",
    "  comb_data = []\n",
    "  for path in data_path_array:\n",
    "    data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    data_tensor_array.append(torch.reshape(torch.tensor(data), (1, len(data),1 )))\n",
    "    comb_data.append(data.tolist())\n",
    "\n",
    "  # export onnx file\n",
    "  export_onnx(prover_model, data_tensor_array, prover_model_path)\n",
    "\n",
    "  # not necessarily be the same path as in prover_gen_settings, but to make it not confused, make it the same\n",
    "  comb_data_path = os.path.join('shared/comb_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "  res = ezkl.compile_circuit(prover_model_path, prover_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "  # now generate the witness file\n",
    "  print('==== Generating Witness ====')\n",
    "  witness = ezkl.gen_witness(comb_data_path, prover_compiled_model_path, witness_path)\n",
    "  assert os.path.isfile(witness_path)\n",
    "  # print(witness[\"outputs\"])\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "  print(\"witness boolean: \", ezkl.vecu64_to_float(witness['outputs'][0][0], output_scale[0]))\n",
    "  for i in range(len(witness['outputs'][1])):\n",
    "    print(\"witness result\", i+1,\":\", ezkl.vecu64_to_float(witness['outputs'][1][i], output_scale[1]))\n",
    "\n",
    "  # GENERATE A PROOF\n",
    "  print(\"==== Generating Proof ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.prove(\n",
    "        witness_path,\n",
    "        prover_compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "        srs_path,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "  print(\"proof: \" ,res)\n",
    "  end_time = time.time()\n",
    "  time_gen_prf = end_time -start_time\n",
    "  print(f\"Time gen prf: {time_gen_prf} seconds\")\n",
    "  assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_verify(proof_path, settings_path, vk_path, srs_path):\n",
    "  # enforce boolean statement to be true\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "\n",
    "  proof = json.load(open(proof_path))\n",
    "  num_inputs = len(settings['model_input_scales'])\n",
    "  print(\"num_inputs: \", num_inputs)\n",
    "  proof[\"instances\"][0][num_inputs] = ezkl.float_to_vecu64(1.0, output_scale[0])\n",
    "  json.dump(proof, open(proof_path, 'w'))\n",
    "\n",
    "  print(\"prf instances: \", proof['instances'])\n",
    "\n",
    "  print(\"proof boolean: \", ezkl.vecu64_to_float(proof['instances'][0][num_inputs], output_scale[0]))\n",
    "  for i in range(num_inputs+1, len(proof['instances'][0])):\n",
    "    print(\"proof result\",i-num_inputs,\":\", ezkl.vecu64_to_float(proof['instances'][0][i], output_scale[1]))\n",
    "\n",
    "\n",
    "  res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "  assert res == True\n",
    "  print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "os.makedirs(os.path.dirname('shared/'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname('prover/'), exist_ok=True)\n",
    "verifier_model_path = os.path.join('shared/verifier.onnx')\n",
    "prover_model_path = os.path.join('prover/prover.onnx')\n",
    "verifier_compiled_model_path = os.path.join('shared/verifier.compiled')\n",
    "prover_compiled_model_path = os.path.join('prover/prover.compiled')\n",
    "pk_path = os.path.join('shared/test.pk')\n",
    "vk_path = os.path.join('shared/test.vk')\n",
    "proof_path = os.path.join('shared/test.pf')\n",
    "settings_path = os.path.join('shared/settings.json')\n",
    "srs_path = os.path.join('shared/kzg.srs')\n",
    "witness_path = os.path.join('prover/witness.json')\n",
    "# comb_data_path = os.path.join('prover/comb_data.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================  ZK-STATS FLOW ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data.json')\n",
    "dummy_data_path = os.path.join('shared/dummy_data.json')\n",
    "\n",
    "f_raw_input = open(data_path, \"r\")\n",
    "data = json.loads(f_raw_input.read())[\"input_data\"][0]\n",
    "data_tensor = torch.reshape(torch.tensor(data),(1, len(data), 1))\n",
    "\n",
    "#  dummy data for data consumer: make the bound approx same as real data\n",
    "dummy_data = np.random.uniform(min(data), max(data), len(data))\n",
    "json.dump({\"input_data\":[dummy_data.tolist()]}, open(dummy_data_path, 'w'))\n",
    "\n",
    "dummy_data_tensor = torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 ))\n",
    "dummy_theory_output = torch.mean(dummy_data_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier/ data consumer side: send desired calculation\n",
    "class verifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(verifier_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = dummy_theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs(torch.sum(X)-X.size()[1]*(self.w))<0.01*torch.sum(X), self.w)\n",
    "    \n",
    "verifier_define_calculation(verifier_model, verifier_model_path, [dummy_data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generate & Calibrate Setting ====\n",
      "scale:  [0]\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":0,\"param_scale\":0,\"scale_rebase_multiplier\":10,\"lookup_range\":[-2332,36068],\"logrows\":16,\"num_inner_cols\":1,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":14432,\"total_assignments\":609,\"total_const_size\":1,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,0],\"model_input_scales\":[0],\"module_sizes\":{\"kzg\":[],\"poseidon\":[14432,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Div\":{\"denom\":100.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.0.8\",\"num_blinding_factors\":null}\n"
     ]
    }
   ],
   "source": [
    "# prover calculates settings, send to verifier\n",
    "prover_gen_settings([data_path], verifier_model_path, [0], \"resources\", settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== setting up ezkl ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time setup: 5.484295129776001 seconds\n",
      "=======================================\n",
      "Theory_output:  tensor(49.9700)\n",
      "==== Generating Witness ====\n",
      "witness boolean:  1.0\n",
      "witness result 1 : 50.0\n",
      "==== Generating Proof ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proof:  {'instances': [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [18373208119716085496, 9977853902793730609, 17093442942004944878, 1769661249493325049]]], 'proof': '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', 'transcript_type': 'EVM'}\n",
      "Time gen prf: 6.585733890533447 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here verifier & prover can concurrently call setup since all params are public to get pk. \n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path )\n",
    "\n",
    "print(\"=======================================\")\n",
    "# Prover generates proof\n",
    "theory_output = torch.mean(data_tensor)\n",
    "print(\"Theory_output: \", theory_output)\n",
    "class prover_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prover_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs(torch.sum(X)-X.size()[1]*(self.w))<0.01*torch.sum(X), self.w)\n",
    "prover_gen_proof(prover_model, [data_path], witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path, pk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_inputs:  1\n",
      "prf instances:  [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [18373208119716085496, 9977853902793730609, 17093442942004944878, 1769661249493325049]]]\n",
      "proof boolean:  1.0\n",
      "proof result 1 : 50.0\n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "# Verifier verifies\n",
    "verifier_verify(proof_path, settings_path, vk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
