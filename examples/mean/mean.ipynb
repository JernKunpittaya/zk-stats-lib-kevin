{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==5.0.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 1)) (5.0.8)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 4)) (1.11.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 5)) (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: statistics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 7)) (1.0.3.5)\n",
      "Requirement already satisfied: onnx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (4.45.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from statistics->-r ../../requirements.txt (line 7)) (0.20.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnx->-r ../../requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->-r ../../requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->-r ../../requirements.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "def export_onnx(model, data_tensor_array, model_loc):\n",
    "  circuit = model()\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  # print(device)\n",
    "\n",
    "  circuit.to(device)\n",
    "\n",
    "  # Flips the neural net into inference mode\n",
    "  circuit.eval()\n",
    "  input_names = []\n",
    "  dynamic_axes = {}\n",
    "\n",
    "  data_tensor_tuple = ()\n",
    "  for i in range(len(data_tensor_array)):\n",
    "    data_tensor_tuple += (data_tensor_array[i],)\n",
    "    input_index = \"input\"+str(i+1)\n",
    "    input_names.append(input_index)\n",
    "    dynamic_axes[input_index] = {0 : 'batch_size'}\n",
    "  dynamic_axes[\"output\"] = {0 : 'batch_size'}\n",
    "\n",
    "  # Export the model\n",
    "  torch.onnx.export(circuit,               # model being run\n",
    "                      data_tensor_tuple,                   # model input (or a tuple for multiple inputs)\n",
    "                      model_loc,            # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=11,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = input_names,   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode is either \"accuracy\" or \"resources\"\n",
    "\n",
    "def gen_settings(comb_data_path, onnx_filename, scale, mode, settings_filename):\n",
    "  print(\"==== Generate & Calibrate Setting ====\")\n",
    "  # Set input to be Poseidon Hash, and param of computation graph to be public\n",
    "  # Poseidon is not homomorphic additive, maybe consider Pedersens or Dory commitment.\n",
    "  gip_run_args = ezkl.PyRunArgs()\n",
    "  gip_run_args.input_visibility = \"hashed\"  # matrix and generalized inverse commitments\n",
    "  gip_run_args.output_visibility = \"public\"   # no parameters used\n",
    "  gip_run_args.param_visibility = \"private\" # should be Tensor(True)--> to enforce arbitrary data in w\n",
    "\n",
    " # generate settings\n",
    "  ezkl.gen_settings(onnx_filename, settings_filename, py_run_args=gip_run_args)\n",
    "  if scale ==\"default\":\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode)\n",
    "  else:\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode, scales = scale)\n",
    "\n",
    "  assert os.path.exists(settings_filename)\n",
    "  assert os.path.exists(comb_data_path)\n",
    "  assert os.path.exists(onnx_filename)\n",
    "  f_setting = open(settings_filename, \"r\")\n",
    "  print(\"scale: \", scale)\n",
    "  print(\"setting: \", f_setting.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_define_calculation(verifier_model, verifier_model_path, dummy_data_path_array):\n",
    "  # load data from dummy_data_path_array into dummy_data_array\n",
    "  dummy_data_tensor_array = []\n",
    "  # comb_dummy_data = []\n",
    "  for path in dummy_data_path_array:\n",
    "    dummy_data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    dummy_data_tensor_array.append(torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 )))\n",
    "    # comb_dummy_data.append(dummy_data.tolist())\n",
    "  # export onnx file\n",
    "  export_onnx(verifier_model,dummy_data_tensor_array, verifier_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we decide to not have comb_data_path as parameter since a bit redundant parameter.\n",
    "def prover_gen_settings(data_path_array, verifier_model_path, scale, mode, settings_path):\n",
    "    comb_data = []\n",
    "    for path in data_path_array:\n",
    "        data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "        comb_data.append(data.tolist())\n",
    "    # this is private to prover since it contains actual data\n",
    "    comb_data_path = os.path.join('prover/comb_data.json')\n",
    "    # Serialize data into file:\n",
    "    json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "    # gen + calibrate setting\n",
    "    gen_settings(comb_data_path, verifier_model_path, scale, mode, settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here prover can concurrently call this since all params are public to get pk. \n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "def verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path ):\n",
    "  # compile circuit\n",
    "  res = ezkl.compile_circuit(verifier_model_path, verifier_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "\n",
    "  # srs path\n",
    "  res = ezkl.get_srs(srs_path, settings_path)\n",
    "\n",
    "  # setupt vk, pk param for use..... prover can use same pk or can init their own!\n",
    "  print(\"==== setting up ezkl ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.setup(\n",
    "        verifier_compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        srs_path)\n",
    "  end_time = time.time()\n",
    "  time_setup = end_time -start_time\n",
    "  print(f\"Time setup: {time_setup} seconds\")\n",
    "\n",
    "  assert res == True\n",
    "  assert os.path.isfile(vk_path)\n",
    "  assert os.path.isfile(pk_path)\n",
    "  assert os.path.isfile(settings_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prover_gen_proof(prover_model, data_path_array, witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path, pk_path, srs_path):\n",
    "  data_tensor_array = []\n",
    "  comb_data = []\n",
    "  for path in data_path_array:\n",
    "    data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    data_tensor_array.append(torch.reshape(torch.tensor(data), (1, len(data),1 )))\n",
    "    comb_data.append(data.tolist())\n",
    "\n",
    "  # export onnx file\n",
    "  export_onnx(prover_model, data_tensor_array, prover_model_path)\n",
    "\n",
    "  # not necessarily be the same path as in prover_gen_settings, but to make it not confused, make it the same\n",
    "  comb_data_path = os.path.join('shared/comb_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "  res = ezkl.compile_circuit(prover_model_path, prover_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "  # now generate the witness file\n",
    "  print('==== Generating Witness ====')\n",
    "  witness = ezkl.gen_witness(comb_data_path, prover_compiled_model_path, witness_path)\n",
    "  assert os.path.isfile(witness_path)\n",
    "  # print(witness[\"outputs\"])\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "  print(\"witness boolean: \", ezkl.vecu64_to_float(witness['outputs'][0][0], output_scale[0]))\n",
    "  for i in range(len(witness['outputs'][1])):\n",
    "    print(\"witness result\", i+1,\":\", ezkl.vecu64_to_float(witness['outputs'][1][i], output_scale[1]))\n",
    "\n",
    "  # GENERATE A PROOF\n",
    "  print(\"==== Generating Proof ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.prove(\n",
    "        witness_path,\n",
    "        prover_compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "        srs_path,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "  print(\"proof: \" ,res)\n",
    "  end_time = time.time()\n",
    "  time_gen_prf = end_time -start_time\n",
    "  print(f\"Time gen prf: {time_gen_prf} seconds\")\n",
    "  assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_verify(proof_path, settings_path, vk_path, srs_path):\n",
    "  # enforce boolean statement to be true\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "\n",
    "  proof = json.load(open(proof_path))\n",
    "  num_inputs = len(settings['model_input_scales'])\n",
    "  print(\"num_inputs: \", num_inputs)\n",
    "  proof[\"instances\"][0][num_inputs] = ezkl.float_to_vecu64(1.0, output_scale[0])\n",
    "  json.dump(proof, open(proof_path, 'w'))\n",
    "\n",
    "  print(\"prf instances: \", proof['instances'])\n",
    "\n",
    "  print(\"proof boolean: \", ezkl.vecu64_to_float(proof['instances'][0][num_inputs], output_scale[0]))\n",
    "  for i in range(num_inputs+1, len(proof['instances'][0])):\n",
    "    print(\"proof result\",i-num_inputs,\":\", ezkl.vecu64_to_float(proof['instances'][0][i], output_scale[1]))\n",
    "\n",
    "\n",
    "  res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "  assert res == True\n",
    "  print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "os.makedirs(os.path.dirname('shared/'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname('prover/'), exist_ok=True)\n",
    "verifier_model_path = os.path.join('shared/verifier.onnx')\n",
    "prover_model_path = os.path.join('prover/prover.onnx')\n",
    "verifier_compiled_model_path = os.path.join('shared/verifier.compiled')\n",
    "prover_compiled_model_path = os.path.join('prover/prover.compiled')\n",
    "pk_path = os.path.join('shared/test.pk')\n",
    "vk_path = os.path.join('shared/test.vk')\n",
    "proof_path = os.path.join('shared/test.pf')\n",
    "settings_path = os.path.join('shared/settings.json')\n",
    "srs_path = os.path.join('shared/kzg.srs')\n",
    "witness_path = os.path.join('prover/witness.json')\n",
    "# comb_data_path = os.path.join('prover/comb_data.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================  ZK-STATS FLOW ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data.json')\n",
    "dummy_data_path = os.path.join('shared/dummy_data.json')\n",
    "\n",
    "f_raw_input = open(data_path, \"r\")\n",
    "data = json.loads(f_raw_input.read())[\"input_data\"][0]\n",
    "data_tensor = torch.reshape(torch.tensor(data),(1, len(data), 1))\n",
    "\n",
    "#  dummy data for data consumer: make the bound approx same as real data\n",
    "dummy_data = np.random.uniform(min(data), max(data), len(data))\n",
    "json.dump({\"input_data\":[dummy_data.tolist()]}, open(dummy_data_path, 'w'))\n",
    "\n",
    "dummy_data_tensor = torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 ))\n",
    "dummy_theory_output = torch.mean(dummy_data_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier/ data consumer side: send desired calculation\n",
    "class verifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(verifier_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = dummy_theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs(torch.sum(X)-X.size()[1]*(self.w))<0.01*torch.sum(X), self.w)\n",
    "    \n",
    "verifier_define_calculation(verifier_model, verifier_model_path, [dummy_data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generate & Calibrate Setting ====\n",
      "scale:  [0]\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":0,\"param_scale\":0,\"scale_rebase_multiplier\":10,\"lookup_range\":[-2332,36068],\"logrows\":16,\"num_inner_cols\":1,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":14432,\"total_assignments\":609,\"total_const_size\":1,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,0],\"model_input_scales\":[0],\"module_sizes\":{\"kzg\":[],\"poseidon\":[14432,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Div\":{\"denom\":100.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.0.8\",\"num_blinding_factors\":null}\n"
     ]
    }
   ],
   "source": [
    "# prover calculates settings, send to verifier\n",
    "prover_gen_settings([data_path], verifier_model_path, [0], \"resources\", settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== setting up ezkl ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time setup: 5.456499099731445 seconds\n",
      "theory_output:  tensor(49.9700)\n",
      "==== Generating Witness ====\n",
      "witness boolean:  1.0\n",
      "witness result 1 : 50.0\n",
      "==== Generating Proof ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proof:  {'instances': [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [18373208119716085496, 9977853902793730609, 17093442942004944878, 1769661249493325049]]], 'proof': '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', 'transcript_type': 'EVM'}\n",
      "Time gen prf: 6.711989879608154 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here verifier & prover can concurrently call setup since all params are public to get pk. \n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path )\n",
    "\n",
    "# Prover generates proof\n",
    "theory_output = torch.mean(data_tensor)\n",
    "print(\"theory_output: \", theory_output)\n",
    "class prover_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prover_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs(torch.sum(X)-X.size()[1]*(self.w))<0.01*torch.sum(X), self.w)\n",
    "prover_gen_proof(prover_model, [data_path], witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path, pk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_inputs:  1\n",
      "prf instances:  [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [18373208119716085496, 9977853902793730609, 17093442942004944878, 1769661249493325049]]]\n",
      "proof boolean:  1.0\n",
      "proof result 1 : 50.0\n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "# Verifier verifies\n",
    "verifier_verify(proof_path, settings_path, vk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
