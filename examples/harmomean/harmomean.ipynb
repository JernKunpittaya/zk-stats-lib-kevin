{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==5.0.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 1)) (5.0.8)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 4)) (1.11.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 5)) (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: statistics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 7)) (1.0.3.5)\n",
      "Requirement already satisfied: onnx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (4.45.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from statistics->-r ../../requirements.txt (line 7)) (0.20.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnx->-r ../../requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->-r ../../requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->-r ../../requirements.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "def export_onnx(model, data_tensor_array, model_loc):\n",
    "  circuit = model()\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  # print(device)\n",
    "\n",
    "  circuit.to(device)\n",
    "\n",
    "  # Flips the neural net into inference mode\n",
    "  circuit.eval()\n",
    "  input_names = []\n",
    "  dynamic_axes = {}\n",
    "\n",
    "  data_tensor_tuple = ()\n",
    "  for i in range(len(data_tensor_array)):\n",
    "    data_tensor_tuple += (data_tensor_array[i],)\n",
    "    input_index = \"input\"+str(i+1)\n",
    "    input_names.append(input_index)\n",
    "    dynamic_axes[input_index] = {0 : 'batch_size'}\n",
    "  dynamic_axes[\"output\"] = {0 : 'batch_size'}\n",
    "\n",
    "  # Export the model\n",
    "  torch.onnx.export(circuit,               # model being run\n",
    "                      data_tensor_tuple,                   # model input (or a tuple for multiple inputs)\n",
    "                      model_loc,            # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=11,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = input_names,   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode is either \"accuracy\" or \"resources\"\n",
    "\n",
    "def gen_settings(comb_data_path, onnx_filename, scale, mode, settings_filename):\n",
    "  print(\"==== Generate & Calibrate Setting ====\")\n",
    "  # Set input to be Poseidon Hash, and param of computation graph to be public\n",
    "  # Poseidon is not homomorphic additive, maybe consider Pedersens or Dory commitment.\n",
    "  gip_run_args = ezkl.PyRunArgs()\n",
    "  gip_run_args.input_visibility = \"hashed\"  # matrix and generalized inverse commitments\n",
    "  gip_run_args.output_visibility = \"public\"   # no parameters used\n",
    "  gip_run_args.param_visibility = \"private\" # should be Tensor(True)--> to enforce arbitrary data in w\n",
    "\n",
    " # generate settings\n",
    "  ezkl.gen_settings(onnx_filename, settings_filename, py_run_args=gip_run_args)\n",
    "  if scale ==\"default\":\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode)\n",
    "  else:\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode, scales = scale)\n",
    "\n",
    "  assert os.path.exists(settings_filename)\n",
    "  assert os.path.exists(comb_data_path)\n",
    "  assert os.path.exists(onnx_filename)\n",
    "  f_setting = open(settings_filename, \"r\")\n",
    "  print(\"scale: \", scale)\n",
    "  print(\"setting: \", f_setting.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_init(verifier_model, verifier_model_path, verifier_compiled_model_path, dummy_data_path_array, settings_path, srs_path, pk_path, vk_path, scale, mode):\n",
    "\n",
    "  # load data from dummy_data_path_array into dummy_data_array\n",
    "  dummy_data_tensor_array = []\n",
    "  comb_dummy_data = []\n",
    "  for path in dummy_data_path_array:\n",
    "    dummy_data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    dummy_data_tensor_array.append(torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 )))\n",
    "    comb_dummy_data.append(dummy_data.tolist())\n",
    "  # export onnx file\n",
    "  export_onnx(verifier_model,dummy_data_tensor_array, verifier_model_path)\n",
    "\n",
    "  comb_dummy_data_path = os.path.join('generated/comb_dummy_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_dummy_data), open(comb_dummy_data_path, 'w' ))\n",
    "\n",
    "  # gen + calibrate setting\n",
    "  gen_settings(comb_dummy_data_path, verifier_model_path, scale, mode, settings_path)\n",
    "\n",
    "  # compile circuit\n",
    "  res = ezkl.compile_circuit(verifier_model_path, verifier_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "\n",
    "  # srs path\n",
    "  res = ezkl.get_srs(srs_path, settings_path)\n",
    "\n",
    "  # setupt vk, pk param for use..... prover can use same pk or can init their own!\n",
    "  print(\"==== setting up ezkl ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.setup(\n",
    "        verifier_compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        srs_path)\n",
    "  end_time = time.time()\n",
    "  time_setup = end_time -start_time\n",
    "  print(f\"Time setup: {time_setup} seconds\")\n",
    "\n",
    "  assert res == True\n",
    "  assert os.path.isfile(vk_path)\n",
    "  assert os.path.isfile(pk_path)\n",
    "  assert os.path.isfile(settings_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prover_gen(prover_model, data_path_array, witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path):\n",
    "  # load data from data_path\n",
    "  # data = json.loads(open(data_path, \"r\").read())[\"input_data\"][0]\n",
    "  # data_tensor = torch.reshape(torch.tensor(data), (1, len(data),1 ))\n",
    "\n",
    "\n",
    "  data_tensor_array = []\n",
    "  comb_data = []\n",
    "  for path in data_path_array:\n",
    "    data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    data_tensor_array.append(torch.reshape(torch.tensor(data), (1, len(data),1 )))\n",
    "    comb_data.append(data.tolist())\n",
    "\n",
    "  # export onnx file\n",
    "  export_onnx(prover_model, data_tensor_array, prover_model_path)\n",
    "\n",
    "  comb_data_path = os.path.join('generated/comb_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "  res = ezkl.compile_circuit(prover_model_path, prover_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "  # now generate the witness file\n",
    "  print('==== Generating Witness ====')\n",
    "  witness = ezkl.gen_witness(comb_data_path, prover_compiled_model_path, witness_path)\n",
    "  assert os.path.isfile(witness_path)\n",
    "  # print(witness[\"outputs\"])\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "  print(\"witness boolean: \", ezkl.vecu64_to_float(witness['outputs'][0][0], output_scale[0]))\n",
    "  for i in range(len(witness['outputs'][1])):\n",
    "    print(\"witness result\", i+1,\":\", ezkl.vecu64_to_float(witness['outputs'][1][i], output_scale[1]))\n",
    "\n",
    "  # GENERATE A PROOF\n",
    "  print(\"==== Generating Proof ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.prove(\n",
    "        witness_path,\n",
    "        prover_compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "        srs_path,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "  print(\"proof: \" ,res)\n",
    "  end_time = time.time()\n",
    "  time_gen_prf = end_time -start_time\n",
    "  print(f\"Time gen prf: {time_gen_prf} seconds\")\n",
    "  assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_verify(proof_path, settings_path, vk_path, srs_path):\n",
    "  # enforce boolean statement to be true\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "\n",
    "  proof = json.load(open(proof_path))\n",
    "  num_inputs = len(settings['model_input_scales'])\n",
    "  print(\"num_inputs: \", num_inputs)\n",
    "  proof[\"instances\"][0][num_inputs] = ezkl.float_to_vecu64(1.0, output_scale[0])\n",
    "  json.dump(proof, open(proof_path, 'w'))\n",
    "\n",
    "  print(\"prf instances: \", proof['instances'])\n",
    "\n",
    "  print(\"proof boolean: \", ezkl.vecu64_to_float(proof['instances'][0][num_inputs], output_scale[0]))\n",
    "  for i in range(num_inputs+1, len(proof['instances'][0])):\n",
    "    print(\"proof result\",i-num_inputs,\":\", ezkl.vecu64_to_float(proof['instances'][0][i], output_scale[1]))\n",
    "\n",
    "\n",
    "  res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "  assert res == True\n",
    "  print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "os.makedirs(os.path.dirname('generated/'), exist_ok=True)\n",
    "verifier_model_path = os.path.join('generated/verifier.onnx')\n",
    "prover_model_path = os.path.join('generated/prover.onnx')\n",
    "verifier_compiled_model_path = os.path.join('generated/verifier.compiled')\n",
    "prover_compiled_model_path = os.path.join('generated/prover.compiled')\n",
    "pk_path = os.path.join('generated/test.pk')\n",
    "vk_path = os.path.join('generated/test.vk')\n",
    "proof_path = os.path.join('generated/test.pf')\n",
    "settings_path = os.path.join('generated/settings.json')\n",
    "srs_path = os.path.join('generated/kzg.srs')\n",
    "witness_path = os.path.join('generated/witness.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================  ZK-STATS FLOW ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data.json')\n",
    "dummy_data_path = os.path.join('generated/dummy_data.json')\n",
    "\n",
    "f_raw_input = open(data_path, \"r\")\n",
    "data = json.loads(f_raw_input.read())[\"input_data\"][0]\n",
    "data_tensor = torch.reshape(torch.tensor(data),(1, len(data), 1))\n",
    "\n",
    "#  dummy data for data consumer: make the bound approx same as real data\n",
    "dummy_data = np.random.uniform(min(data), max(data), len(data))\n",
    "json.dump({\"input_data\":[dummy_data.tolist()]}, open(dummy_data_path, 'w'))\n",
    "\n",
    "dummy_data_tensor = torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 ))\n",
    "dummy_theory_output = torch.div(1.0,torch.mean(torch.div(1.0,dummy_data_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39.7069, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(dummy_theory_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generate & Calibrate Setting ====\n",
      "scale:  [4]\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":4,\"param_scale\":4,\"scale_rebase_multiplier\":10,\"lookup_range\":[-55810,20990],\"logrows\":17,\"num_inner_cols\":1,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":14432,\"total_assignments\":907,\"total_const_size\":0,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,4],\"model_input_scales\":[4],\"module_sizes\":{\"kzg\":[],\"poseidon\":[14432,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Recip\":{\"scale\":256.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.0.8\",\"num_blinding_factors\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== setting up ezkl ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time setup: 10.03211236000061 seconds\n"
     ]
    }
   ],
   "source": [
    "# Verifier/ data consumer side:\n",
    "# since have division, scale must be very LARGE, cant use just 0\n",
    "# So, we allow error rate to be 10% instead of 1%\n",
    "class verifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(verifier_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = dummy_theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs((self.w*torch.sum(torch.div(1.0,X)))-X.size()[1])<0.1*X.size()[1], self.w)\n",
    "\n",
    "verifier_init(verifier_model, verifier_model_path, verifier_compiled_model_path, [dummy_data_path], settings_path, srs_path, pk_path, vk_path,[6], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theory_output:  tensor(45.2144)\n",
      "==== Generating Witness ====\n",
      "witness boolean:  0.0\n",
      "witness result 1 : 45.1875\n",
      "==== Generating Proof ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proof:  {'instances': [[[13386294066959150413, 908164135785594202, 4171652057457726439, 666153533336791975], [0, 0, 0, 0], [52827337531584784, 8692586534060066330, 15055157321672429687, 2714593037445992654]]], 'proof': '2c333be0c500d99eaea56115d322f2649258d3e7a55a6571bbfd7640dd3c54ae03b082676a29deafe486ee090158b169ee92473ce13d0a2657d565bb88cc8a8d21c110887d319a2062e1f624df66ac871bdcda4cca9814884be90a14a2c01c540213f76f757428c4d1b7a3c8f9ff200e465842435782faa88ec594a181cbb6d52a5819cb678fc760f12ac7aa246077f0a3fb31cf635e8b8a04151b1f90ec89320d20ca421442cd57bcafda8c3e219dc28c695af75151c1c6eebb6830b144e55c2af44ae6764d12af9660945d240643196375dea4949884c730543269804cf39623cc8fcb55461499920580477c1c54d3961fef7fb1f60ae4dec8e98b670b746c2f8175aa7cf5f7e1b117277eac24b482380fff72ea6c67b5bdafb375e95f78901c215164a65d98bd63d049ca51d9d7e2f194998b14c1316cf7bde8fdd433a5ca187b57b51aead8301f75b4e19c46b92f1242ec6ad0f592d8caf007703a490deb0224e55084e4362752334bccd890ceeb25ec3d2e956c1f59b925f83e303539eb2c6285da0f39e6c2e28d19a7de3eea9175fb9c15e2a94d0d760bd3a975962e671b87b557e023d358eac4d89075352e7f4e16b0f24c2bc215d1f3be536b83e6b926f81615cb00c96c948660aca074cb375454f670a633106072fcafb9e8ce3afa142238bea634e39d14eba4a8d4073dd9fba5b58d4eac651d956bd90807da5cf12cc14fff2f435c268dcffe3e036d187b87d75497b34ba0557112f79cdbf8224e265251afa0e245f0105cf167e7b80d7ddc59d00bb4b2a9aaa2593582f4f91f6d17ef06f7f7e7bda7ddab69b782a739478500d88800874e5b2e94b3fd40b310da2b288c48edabae54ab1e8c3bd72e4ac7d018236cf7c7891e9a9a6e31177ced040ca8a98acdc0c39faf445b040691038f86f9a18a17d1ccc98412dc9f8ed4d3690f1fe02ac4bebd2584f8bbc2ae1785d79005c644d36620cecc5281ccd79835d722e10904bc360d568f46caafdac44d044ba9e2e505c358116c31d86c87cf36072f62d7400cbdca782c1d080971e6c4792896bf17e6fcfa6a4d94c7b901fa9747268c38c1b0fa365cedb5b99671c989395dc2042ff7f239234376a562827ef2d01a70b5274c6da3ad9a4cd50800cf25b8d97a0c32bf4e0ce55fd3ab2e9a2156b02effa3f815e936618f70211e5242d996e4f59ffa0696150844a9f8d67f873cbf2a8d3aea584b8a41c1d03c1f4d4f6387591fe88cd89fe052f1cc8f199bd6de0a2a315534a5a94d2cf390741bbaa3dee76efaa5aac70019170c334a3bc94d771d1abc1362e4f476bfbeadfb667480449d7b16169557453b46aa7bc3b76a21c7320b201c1d5c2573e93fe6b6dd69298f70369dd3fa10b18b40bca416d446d8db8206668b26d7583d72e2a43869304b2d01a538015df3a312da40c1272a9a9637280c321c786460820d4bb4833931148db58d9cbc12c58cf87c31774d15faa75d72050761f6e4950c2b849ccc19cf02b56677eed40eb11089aa61adaee382b0a6921596fc84444058ae099631e75fa32ea5720ae3d37021a3363edcf9e033f3ed4816f1df545c6751966ff07b2f23eaa84c2778015a26632c446abc180f42bbc167151f377c86925ddbc6b4bb3d3699ed69dbfea3adc019e40ec6080b8362f06afa07394f3936c574f7b161603e6e80afbbe36bbf835688565c0beb7385806507950feef6c75f99ce1643fe61c345bd1ed454c364fb77a960ff28599ec92aa7fb7a07687ac3d619ef74f3b3793855204b20d360b87fef0d7d15769feec037ea2fc100ad9c07939c2437f65644a8fd6c6ccb5ca1aeed17a146e39e48030bda3962ab144e5604474ad0d1eb818b1d658817f3626f18f7f64d56ebf4deb51a92e393cf06b057d58d793f42391c5c5f80fdf2e71c6700df51370fed09477d473f0c0dd21bfb9539f84050e5347ea9c0c6c6b6046afc23dec44c485e8d12366da637d3c01b9db459c36d93efd98694c75f85155f8b5bbb8657ead94927db4b997978882a1844c808e8a476b1377c554dfc44bf5952efecf2835be4a56b0eafa16dacfdd2254bb405bf387c677dbe7b83867e9fda9842d8561014d0311063461fce8f852e21608fdd113f5f98be1f8b4a0b8bb44e00011157d3e143f435af629f41e03e502e0a3d665a21bb9825fa94b3592c3a617cf8076eeb2db9604e394799e79f58e11f236d3c3ac8bd9fd52ec3283592e9f7e3df970993468aa1a546d6133d8358f32b6591757bece4bafe14782746829838d5da21961861441d1cb278f71446e1962ef2e26a6b6667a385dfe0281820e07d43ba0016a7c22a9ab56d97712f3675d00d0bd00956830dc7957b84c46bcbf90faf8115d2d216a9ddedc975569cd2124e25498cbac47b8152dcad82d23751bd9609fd247e5669c896557306156280de691c42f5f1d2892f914fef8d1544f03b310e07b4030c4f648a13e41c9e60d5cc4423ba32e23340f5ddb5dcbcfc2c2d24160981b34d6553691673d31a55c9d821552125954413c969a619c3babe7be0529c4633f9cfd76854479fd8a9d3563a4f9d0d03e256d692e5e9acb085b20c940495dc1876c6f854f99c522c1ae208f802b529c81562ac9d027d2e468c21f6f2971447e97d16fbc80ac24990453c4f9d515f0355c02c51341039bdfe5a5a2742d4da74bc2c943623a009ac872800383a85f515b2608dbe31d3c4e3d6652fd4f518133873ce8cce43e083ef0de48c4d9b59e91861918704bfcd1133e065c9877ea677ab6f4e32869686f477492c439d9a29e329b146b15c2103ac7386c937a44cf6b109eb9d55c40f4ee00c4f8b08bd7589f30098bb2836776230856dcd404b9aed9dbb8342a6706cb55710ac1cfc27ce74b92b84f8a34e438e8af9b04e5fa2b55ff5c53c7f9ca885b561ad1a0a6f03d10a4a17e9d417b8997d7c05d21719ffb879dc0d7876cff452295ecd29d68e0ca21f29069056e1bb060f3592d6701142fe5d89bb0ecf7192279ae302784fece3e82a9e2c9bd91b724cb786a96c926a9525065d3676eab5cb989d372079072e9e08be5f1ec82fd6cf07e901813b69c7a9636920fdee0b025917dabf2fb36c628eb3680b2d3c15445944918919d15d0aa3190c9ba16039851a79f74b1774097124a36a5e09be9517d65eea1a7de6faa7aaa9c5137d80147ad299a83512fc76b98873fe87065278a0e472e151f8e56a5f9a0ed918285fc06b6bd6c81ab16b22b4354d4ec519e30ed8dc3a55c8cc837e8d95a107cfc4ea8cc05a76f54236e17d6591f8ab48031f1e9d47475fa1e8a062e552c2089becdb0d177abc07ccd12f579c7c3fe025134fb4579631dda6d0971c60eabef6bbc98546baf45b1842ef06818a66f3ed9920ede8b4659a99b44c908f479f8009459666005ddf58629b70b74d5b4541f19f140f3b5640a0828c87f40f8d50292c572f5314eab992164f96e7c77e1cf7992b0f84d1ffa2d1e6a812b5f2faba803d581dde91d8d2c4ab7a3f91b27fc1af750018c85aeed0dee37e944981788dfeb66aacb25e7228af509fa182ae9f3f79a7d50cf492fc1b7a17228a08616a24f0bf957a5c2dd303fae7744328c898fc08cb0e1dc3dfaf9666ebd19af9c7f1ace0e238fb3924d0e54ea54b83106d2f8f89831105eaddc7cccdde11fcb668d0523d0b8189dd363c2fa2ca58d6179ed711175bba1769650cacdd5b6bb3fca732fe2a25d2d102f057babe6a2584ae31ef322ddb0826d5d2491b30e851186680f50ac8503fd497920eaac31e70628e3742fab309b329cd2ec09b7c8bd8a9a8b08f5a56b18c8947c410cb8c65d4db6430a5318bcaa0229cad5acd215485734ea301f4ccc180f2c23b2e0caf760dccb70039c147bb5f06b69844f2ad0d55c3684cfcf6d3546b27a39dd1835b04cac131ed7b4269a4542e5341a5e2c7b9a92067d11e837c8f51352eaef8d21b96b97eaa9639904289c3254e95779029ee49b77c30a5c38794d13747f43ec79a2214789b076ab5313b4411d611fdc3aadecc55d6d12d291884211fcdb8aee8a25fcaf05bcc75e4bd26ff2d44cbf01aea2d8bb72acad734fe1686b6fd4f856c1d212cc78b26b508695ae9198febe5a4b7c9a946b3402a7ca15ce68eecf919c55f85c5c272d4d5b006b4e72a30511a5a8d47692f10c7d8f5b28ae1b6abcb2c9eb3fdf1dec388a11c850038189dfc6af65ed7ba460b8902797f284b68a70e677ec72978981f102a80b22913237b48d672e12ce180f1f14e2b8ed1730efb0aad7a84d46b5bd7246911bf59f2235646b6fc28d28d5a48cbe089d98c10a3f56c0e162009f17438c72e0687553a0d6018dd28ecec4b77c93242715e9c669b68177d40d487c95343649a82d0e3f129488c060a0b2616e941e21e31f1865a69d572e083390983915d7c4464d6462925281a8beb0856d4a2b0c893dd3ac129de0e9e33ee6c11d734fb603623c642a40072db93c08bc1f374b39a45c1ac3c773c045b364f70e3507b52e1c08d74229919e18e9656b5018968c04c89b8bcacbed1b06d6a46e90bde1a5b867d3f2270b10e00b5afbc1e3ed39632b6dc6c8aff7bd1c1944a1e73551a848f24f063a2f63a', 'transcript_type': 'EVM'}\n",
      "Time gen prf: 12.713093996047974 seconds\n"
     ]
    }
   ],
   "source": [
    "# Prover/ data owner side\n",
    "theory_output = torch.div(1.0,torch.mean(torch.div(1.0,data_tensor)))\n",
    "print(\"theory_output: \",theory_output)\n",
    "class prover_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prover_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs((self.w*torch.sum(torch.div(1.0,X)))-X.size()[1])<0.1*X.size()[1], self.w)\n",
    "prover_gen(prover_model, [data_path], witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_inputs:  1\n",
      "prf instances:  [[[13386294066959150413, 908164135785594202, 4171652057457726439, 666153533336791975], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [52827337531584784, 8692586534060066330, 15055157321672429687, 2714593037445992654]]]\n",
      "proof boolean:  1.0\n",
      "proof result 1 : 45.1875\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run verify: The constraint system is not satisfied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Verifier verifies\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m verifier_verify(proof_path, settings_path, vk_path, srs_path)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtheory mean: \u001b[39m\u001b[39m\"\u001b[39m, theory_output)\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mverifier_verify\u001b[0;34m(proof_path, settings_path, vk_path, srs_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_inputs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(proof[\u001b[39m'\u001b[39m\u001b[39minstances\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])):\n\u001b[1;32m     16\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mproof result\u001b[39m\u001b[39m\"\u001b[39m,i\u001b[39m-\u001b[39mnum_inputs,\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m, ezkl\u001b[39m.\u001b[39mvecu64_to_float(proof[\u001b[39m'\u001b[39m\u001b[39minstances\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][i], output_scale[\u001b[39m1\u001b[39m]))\n\u001b[0;32m---> 19\u001b[0m res \u001b[39m=\u001b[39m ezkl\u001b[39m.\u001b[39;49mverify(\n\u001b[1;32m     20\u001b[0m       proof_path,\n\u001b[1;32m     21\u001b[0m       settings_path,\n\u001b[1;32m     22\u001b[0m       vk_path,\n\u001b[1;32m     23\u001b[0m       srs_path,\n\u001b[1;32m     24\u001b[0m   )\n\u001b[1;32m     26\u001b[0m \u001b[39massert\u001b[39;00m res \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mverified\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run verify: The constraint system is not satisfied"
     ]
    }
   ],
   "source": [
    "# Verifier verifies\n",
    "verifier_verify(proof_path, settings_path, vk_path, srs_path)\n",
    "print(\"theory mean: \", theory_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
