{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==5.0.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 1)) (5.0.8)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 4)) (1.11.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 5)) (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: statistics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 7)) (1.0.3.5)\n",
      "Requirement already satisfied: onnx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (4.45.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from statistics->-r ../../requirements.txt (line 7)) (0.20.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnx->-r ../../requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->-r ../../requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->-r ../../requirements.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "def export_onnx(model, data_tensor_array, model_loc):\n",
    "  circuit = model()\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  # print(device)\n",
    "\n",
    "  circuit.to(device)\n",
    "\n",
    "  # Flips the neural net into inference mode\n",
    "  circuit.eval()\n",
    "  input_names = []\n",
    "  dynamic_axes = {}\n",
    "\n",
    "  data_tensor_tuple = ()\n",
    "  for i in range(len(data_tensor_array)):\n",
    "    data_tensor_tuple += (data_tensor_array[i],)\n",
    "    input_index = \"input\"+str(i+1)\n",
    "    input_names.append(input_index)\n",
    "    dynamic_axes[input_index] = {0 : 'batch_size'}\n",
    "  dynamic_axes[\"output\"] = {0 : 'batch_size'}\n",
    "\n",
    "  # Export the model\n",
    "  torch.onnx.export(circuit,               # model being run\n",
    "                      data_tensor_tuple,                   # model input (or a tuple for multiple inputs)\n",
    "                      model_loc,            # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=11,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = input_names,   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode is either \"accuracy\" or \"resources\"\n",
    "\n",
    "def gen_settings(comb_data_path, onnx_filename, scale, mode, settings_filename):\n",
    "  print(\"==== Generate & Calibrate Setting ====\")\n",
    "  # Set input to be Poseidon Hash, and param of computation graph to be public\n",
    "  # Poseidon is not homomorphic additive, maybe consider Pedersens or Dory commitment.\n",
    "  gip_run_args = ezkl.PyRunArgs()\n",
    "  gip_run_args.input_visibility = \"hashed\"  # matrix and generalized inverse commitments\n",
    "  gip_run_args.output_visibility = \"public\"   # no parameters used\n",
    "  gip_run_args.param_visibility = \"private\" # should be Tensor(True)--> to enforce arbitrary data in w\n",
    "\n",
    " # generate settings\n",
    "  ezkl.gen_settings(onnx_filename, settings_filename, py_run_args=gip_run_args)\n",
    "  if scale ==\"default\":\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode)\n",
    "  else:\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode, scales = scale)\n",
    "\n",
    "  assert os.path.exists(settings_filename)\n",
    "  assert os.path.exists(comb_data_path)\n",
    "  assert os.path.exists(onnx_filename)\n",
    "  f_setting = open(settings_filename, \"r\")\n",
    "  print(\"scale: \", scale)\n",
    "  print(\"setting: \", f_setting.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_init(verifier_model, verifier_model_path, verifier_compiled_model_path, dummy_data_path_array, settings_path, srs_path, pk_path, vk_path, scale, mode):\n",
    "\n",
    "  # load data from dummy_data_path_array into dummy_data_array\n",
    "  dummy_data_tensor_array = []\n",
    "  comb_dummy_data = []\n",
    "  for path in dummy_data_path_array:\n",
    "    dummy_data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    dummy_data_tensor_array.append(torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 )))\n",
    "    comb_dummy_data.append(dummy_data.tolist())\n",
    "  # export onnx file\n",
    "  export_onnx(verifier_model,dummy_data_tensor_array, verifier_model_path)\n",
    "\n",
    "  comb_dummy_data_path = os.path.join('generated/comb_dummy_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_dummy_data), open(comb_dummy_data_path, 'w' ))\n",
    "\n",
    "  # gen + calibrate setting\n",
    "  gen_settings(comb_dummy_data_path, verifier_model_path, scale, mode, settings_path)\n",
    "\n",
    "  # compile circuit\n",
    "  res = ezkl.compile_circuit(verifier_model_path, verifier_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "\n",
    "  # srs path\n",
    "  res = ezkl.get_srs(srs_path, settings_path)\n",
    "\n",
    "  # setupt vk, pk param for use..... prover can use same pk or can init their own!\n",
    "  print(\"==== setting up ezkl ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.setup(\n",
    "        verifier_compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        srs_path)\n",
    "  end_time = time.time()\n",
    "  time_setup = end_time -start_time\n",
    "  print(f\"Time setup: {time_setup} seconds\")\n",
    "\n",
    "  assert res == True\n",
    "  assert os.path.isfile(vk_path)\n",
    "  assert os.path.isfile(pk_path)\n",
    "  assert os.path.isfile(settings_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prover_gen(prover_model, data_path_array, witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path):\n",
    "  # load data from data_path\n",
    "  # data = json.loads(open(data_path, \"r\").read())[\"input_data\"][0]\n",
    "  # data_tensor = torch.reshape(torch.tensor(data), (1, len(data),1 ))\n",
    "\n",
    "\n",
    "  data_tensor_array = []\n",
    "  comb_data = []\n",
    "  for path in data_path_array:\n",
    "    data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    data_tensor_array.append(torch.reshape(torch.tensor(data), (1, len(data),1 )))\n",
    "    comb_data.append(data.tolist())\n",
    "\n",
    "  # export onnx file\n",
    "  export_onnx(prover_model, data_tensor_array, prover_model_path)\n",
    "\n",
    "  comb_data_path = os.path.join('generated/comb_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "  res = ezkl.compile_circuit(prover_model_path, prover_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "  # now generate the witness file\n",
    "  print('==== Generating Witness ====')\n",
    "  witness = ezkl.gen_witness(comb_data_path, prover_compiled_model_path, witness_path)\n",
    "  assert os.path.isfile(witness_path)\n",
    "  # print(witness[\"outputs\"])\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "  print(\"witness boolean: \", ezkl.vecu64_to_float(witness['outputs'][0][0], output_scale[0]))\n",
    "  for i in range(len(witness['outputs'][1])):\n",
    "    print(\"witness result\", i+1,\":\", ezkl.vecu64_to_float(witness['outputs'][1][i], output_scale[1]))\n",
    "\n",
    "  # GENERATE A PROOF\n",
    "  print(\"==== Generating Proof ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.prove(\n",
    "        witness_path,\n",
    "        prover_compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "        srs_path,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "  print(\"proof: \" ,res)\n",
    "  end_time = time.time()\n",
    "  time_gen_prf = end_time -start_time\n",
    "  print(f\"Time gen prf: {time_gen_prf} seconds\")\n",
    "  assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_verify(proof_path, settings_path, vk_path, srs_path):\n",
    "  # enforce boolean statement to be true\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "\n",
    "  proof = json.load(open(proof_path))\n",
    "  num_inputs = len(settings['model_input_scales'])\n",
    "  print(\"num_inputs: \", num_inputs)\n",
    "  proof[\"instances\"][0][num_inputs] = ezkl.float_to_vecu64(1.0, output_scale[0])\n",
    "  json.dump(proof, open(proof_path, 'w'))\n",
    "\n",
    "  print(\"prf instances: \", proof['instances'])\n",
    "\n",
    "  print(\"proof boolean: \", ezkl.vecu64_to_float(proof['instances'][0][num_inputs], output_scale[0]))\n",
    "  for i in range(num_inputs+1, len(proof['instances'][0])):\n",
    "    print(\"proof result\",i-num_inputs,\":\", ezkl.vecu64_to_float(proof['instances'][0][i], output_scale[1]))\n",
    "\n",
    "\n",
    "  res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "  assert res == True\n",
    "  print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "os.makedirs(os.path.dirname('generated/'), exist_ok=True)\n",
    "verifier_model_path = os.path.join('generated/verifier.onnx')\n",
    "prover_model_path = os.path.join('generated/prover.onnx')\n",
    "verifier_compiled_model_path = os.path.join('generated/verifier.compiled')\n",
    "prover_compiled_model_path = os.path.join('generated/prover.compiled')\n",
    "pk_path = os.path.join('generated/test.pk')\n",
    "vk_path = os.path.join('generated/test.vk')\n",
    "proof_path = os.path.join('generated/test.pf')\n",
    "settings_path = os.path.join('generated/settings.json')\n",
    "srs_path = os.path.join('generated/kzg.srs')\n",
    "witness_path = os.path.join('generated/witness.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================  ZK-STATS FLOW ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data.json')\n",
    "dummy_data_path = os.path.join('generated/dummy_data.json')\n",
    "\n",
    "f_raw_input = open(data_path, \"r\")\n",
    "data = json.loads(f_raw_input.read())[\"input_data\"][0]\n",
    "data_tensor = torch.reshape(torch.tensor(data),(1, len(data), 1))\n",
    "\n",
    "#  dummy data for data consumer: make the bound approx same as real data\n",
    "dummy_data = np.random.uniform(min(data), max(data), len(data))\n",
    "json.dump({\"input_data\":[dummy_data.tolist()]}, open(dummy_data_path, 'w'))\n",
    "\n",
    "dummy_data_tensor = torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_theory_output = torch.sqrt(torch.var(dummy_data_tensor, correction = 1))\n",
    "dummy_data_mean = torch.mean(dummy_data_tensor)\n",
    "# print(dummy_theory_output)\n",
    "# print(torch.sum(torch.pow(dummy_data_tensor-torch.mean(dummy_data_tensor), 2))/(dummy_data_tensor.size()[1]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:2174: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Bool' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
      "  return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generate & Calibrate Setting ====\n",
      "scale:  [0]\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":0,\"param_scale\":0,\"scale_rebase_multiplier\":10,\"lookup_range\":[-610,41960],\"logrows\":16,\"num_inner_cols\":1,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":14432,\"total_assignments\":1515,\"total_const_size\":1,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,0],\"model_input_scales\":[0],\"module_sizes\":{\"kzg\":[],\"poseidon\":[14432,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Div\":{\"denom\":100.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.0.8\",\"num_blinding_factors\":null}\n",
      "==== setting up ezkl ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time setup: 5.680230140686035 seconds\n"
     ]
    }
   ],
   "source": [
    "# Verifier/ data consumer side:\n",
    "class verifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(verifier_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = dummy_theory_output, requires_grad = False)\n",
    "        self.data_mean = nn.Parameter(data = dummy_data_mean, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.logical_and(torch.abs(torch.sum((X-self.data_mean)*(X-self.data_mean))-self.w*self.w*(X.size()[1]-1))<0.01*self.w*self.w*(X.size()[1]-1),torch.abs(torch.sum(X)-X.size()[1]*(self.data_mean))<0.01*torch.sum(X) ),self.w)\n",
    "\n",
    "verifier_init(verifier_model, verifier_model_path, verifier_compiled_model_path, [dummy_data_path], settings_path, srs_path, pk_path, vk_path,[0], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theory output:  tensor(14.5800)\n",
      "==== Generating Witness ====\n",
      "witness boolean:  1.0\n",
      "witness result 1 : 15.0\n",
      "==== Generating Proof ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:2174: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Bool' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
      "  return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))\n",
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proof:  {'instances': [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [956231351009279921, 10951436676983309100, 2250248050743556928, 1228298028208591648]]], 'proof': '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', 'transcript_type': 'EVM'}\n",
      "Time gen prf: 6.705816984176636 seconds\n"
     ]
    }
   ],
   "source": [
    "# Prover/ data owner side\n",
    "theory_output = torch.sqrt(torch.var(data_tensor, correction = 1))\n",
    "data_mean = torch.mean(data_tensor)\n",
    "print(\"theory output: \", theory_output)\n",
    "class prover_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prover_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = theory_output, requires_grad = False)\n",
    "        self.data_mean = nn.Parameter(data = data_mean, requires_grad = False)\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "       return (torch.logical_and(torch.abs(torch.sum((X-self.data_mean)*(X-self.data_mean))-self.w*self.w*(X.size()[1]-1))<0.01*self.w*self.w*(X.size()[1]-1),torch.abs(torch.sum(X)-X.size()[1]*(self.data_mean))<0.01*torch.sum(X)),self.w)\n",
    "prover_gen(prover_model, [data_path], witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_inputs:  1\n",
      "prf instances:  [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [956231351009279921, 10951436676983309100, 2250248050743556928, 1228298028208591648]]]\n",
      "proof boolean:  1.0\n",
      "proof result 1 : 15.0\n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "# Verifier verifies\n",
    "verifier_verify(proof_path, settings_path, vk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
