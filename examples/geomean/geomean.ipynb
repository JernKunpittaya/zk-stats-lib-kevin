{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==5.0.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 1)) (5.0.8)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 4)) (1.11.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 5)) (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: statistics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 7)) (1.0.3.5)\n",
      "Requirement already satisfied: onnx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (4.45.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from statistics->-r ../../requirements.txt (line 7)) (0.20.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnx->-r ../../requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->-r ../../requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->-r ../../requirements.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "def export_onnx(model, data_tensor_array, model_loc):\n",
    "  circuit = model()\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  # print(device)\n",
    "\n",
    "  circuit.to(device)\n",
    "\n",
    "  # Flips the neural net into inference mode\n",
    "  circuit.eval()\n",
    "  input_names = []\n",
    "  dynamic_axes = {}\n",
    "\n",
    "  data_tensor_tuple = ()\n",
    "  for i in range(len(data_tensor_array)):\n",
    "    data_tensor_tuple += (data_tensor_array[i],)\n",
    "    input_index = \"input\"+str(i+1)\n",
    "    input_names.append(input_index)\n",
    "    dynamic_axes[input_index] = {0 : 'batch_size'}\n",
    "  dynamic_axes[\"output\"] = {0 : 'batch_size'}\n",
    "\n",
    "  # Export the model\n",
    "  torch.onnx.export(circuit,               # model being run\n",
    "                      data_tensor_tuple,                   # model input (or a tuple for multiple inputs)\n",
    "                      model_loc,            # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=11,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = input_names,   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode is either \"accuracy\" or \"resources\"\n",
    "\n",
    "def gen_settings(comb_data_path, onnx_filename, scale, mode, settings_filename):\n",
    "  print(\"==== Generate & Calibrate Setting ====\")\n",
    "  # Set input to be Poseidon Hash, and param of computation graph to be public\n",
    "  # Poseidon is not homomorphic additive, maybe consider Pedersens or Dory commitment.\n",
    "  gip_run_args = ezkl.PyRunArgs()\n",
    "  gip_run_args.input_visibility = \"hashed\"  # matrix and generalized inverse commitments\n",
    "  gip_run_args.output_visibility = \"public\"   # no parameters used\n",
    "  gip_run_args.param_visibility = \"private\" # should be Tensor(True)--> to enforce arbitrary data in w\n",
    "\n",
    " # generate settings\n",
    "  ezkl.gen_settings(onnx_filename, settings_filename, py_run_args=gip_run_args)\n",
    "  if scale ==\"default\":\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode)\n",
    "  else:\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode, scales = scale)\n",
    "\n",
    "  assert os.path.exists(settings_filename)\n",
    "  assert os.path.exists(comb_data_path)\n",
    "  assert os.path.exists(onnx_filename)\n",
    "  f_setting = open(settings_filename, \"r\")\n",
    "  print(\"scale: \", scale)\n",
    "  print(\"setting: \", f_setting.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_define_calculation(verifier_model, verifier_model_path, dummy_data_path_array):\n",
    "  # load data from dummy_data_path_array into dummy_data_array\n",
    "  dummy_data_tensor_array = []\n",
    "  # comb_dummy_data = []\n",
    "  for path in dummy_data_path_array:\n",
    "    dummy_data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    dummy_data_tensor_array.append(torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 )))\n",
    "    # comb_dummy_data.append(dummy_data.tolist())\n",
    "  # export onnx file\n",
    "  export_onnx(verifier_model,dummy_data_tensor_array, verifier_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we decide to not have comb_data_path as parameter since a bit redundant parameter.\n",
    "def prover_gen_settings(data_path_array, verifier_model_path, scale, mode, settings_path):\n",
    "    comb_data = []\n",
    "    for path in data_path_array:\n",
    "        data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "        comb_data.append(data.tolist())\n",
    "    # this is private to prover since it contains actual data\n",
    "    comb_data_path = os.path.join('prover/comb_data.json')\n",
    "    # Serialize data into file:\n",
    "    json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "    # gen + calibrate setting\n",
    "    gen_settings(comb_data_path, verifier_model_path, scale, mode, settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here prover can concurrently call this since all params are public to get pk. \n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "def verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path ):\n",
    "  # compile circuit\n",
    "  res = ezkl.compile_circuit(verifier_model_path, verifier_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "\n",
    "  # srs path\n",
    "  res = ezkl.get_srs(srs_path, settings_path)\n",
    "\n",
    "  # setupt vk, pk param for use..... prover can use same pk or can init their own!\n",
    "  print(\"==== setting up ezkl ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.setup(\n",
    "        verifier_compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        srs_path)\n",
    "  end_time = time.time()\n",
    "  time_setup = end_time -start_time\n",
    "  print(f\"Time setup: {time_setup} seconds\")\n",
    "\n",
    "  assert res == True\n",
    "  assert os.path.isfile(vk_path)\n",
    "  assert os.path.isfile(pk_path)\n",
    "  assert os.path.isfile(settings_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prover_gen_proof(prover_model, data_path_array, witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path, pk_path, srs_path):\n",
    "  data_tensor_array = []\n",
    "  comb_data = []\n",
    "  for path in data_path_array:\n",
    "    data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    data_tensor_array.append(torch.reshape(torch.tensor(data), (1, len(data),1 )))\n",
    "    comb_data.append(data.tolist())\n",
    "\n",
    "  # export onnx file\n",
    "  export_onnx(prover_model, data_tensor_array, prover_model_path)\n",
    "\n",
    "  # not necessarily be the same path as in prover_gen_settings, but to make it not confused, make it the same\n",
    "  comb_data_path = os.path.join('shared/comb_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "  res = ezkl.compile_circuit(prover_model_path, prover_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "  # now generate the witness file\n",
    "  print('==== Generating Witness ====')\n",
    "  witness = ezkl.gen_witness(comb_data_path, prover_compiled_model_path, witness_path)\n",
    "  assert os.path.isfile(witness_path)\n",
    "  # print(witness[\"outputs\"])\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "  print(\"witness boolean: \", ezkl.vecu64_to_float(witness['outputs'][0][0], output_scale[0]))\n",
    "  for i in range(len(witness['outputs'][1])):\n",
    "    print(\"witness result\", i+1,\":\", ezkl.vecu64_to_float(witness['outputs'][1][i], output_scale[1]))\n",
    "\n",
    "  # GENERATE A PROOF\n",
    "  print(\"==== Generating Proof ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.prove(\n",
    "        witness_path,\n",
    "        prover_compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "        srs_path,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "  print(\"proof: \" ,res)\n",
    "  end_time = time.time()\n",
    "  time_gen_prf = end_time -start_time\n",
    "  print(f\"Time gen prf: {time_gen_prf} seconds\")\n",
    "  assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_verify(proof_path, settings_path, vk_path, srs_path):\n",
    "  # enforce boolean statement to be true\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "\n",
    "  proof = json.load(open(proof_path))\n",
    "  num_inputs = len(settings['model_input_scales'])\n",
    "  print(\"num_inputs: \", num_inputs)\n",
    "  proof[\"instances\"][0][num_inputs] = ezkl.float_to_vecu64(1.0, output_scale[0])\n",
    "  json.dump(proof, open(proof_path, 'w'))\n",
    "\n",
    "  print(\"prf instances: \", proof['instances'])\n",
    "\n",
    "  print(\"proof boolean: \", ezkl.vecu64_to_float(proof['instances'][0][num_inputs], output_scale[0]))\n",
    "  for i in range(num_inputs+1, len(proof['instances'][0])):\n",
    "    print(\"proof result\",i-num_inputs,\":\", ezkl.vecu64_to_float(proof['instances'][0][i], output_scale[1]))\n",
    "\n",
    "\n",
    "  res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "  assert res == True\n",
    "  print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "os.makedirs(os.path.dirname('shared/'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname('prover/'), exist_ok=True)\n",
    "verifier_model_path = os.path.join('shared/verifier.onnx')\n",
    "prover_model_path = os.path.join('prover/prover.onnx')\n",
    "verifier_compiled_model_path = os.path.join('shared/verifier.compiled')\n",
    "prover_compiled_model_path = os.path.join('prover/prover.compiled')\n",
    "pk_path = os.path.join('shared/test.pk')\n",
    "vk_path = os.path.join('shared/test.vk')\n",
    "proof_path = os.path.join('shared/test.pf')\n",
    "settings_path = os.path.join('shared/settings.json')\n",
    "srs_path = os.path.join('shared/kzg.srs')\n",
    "witness_path = os.path.join('prover/witness.json')\n",
    "# comb_data_path = os.path.join('prover/comb_data.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================  ZK-STATS FLOW ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data.json')\n",
    "dummy_data_path = os.path.join('shared/dummy_data.json')\n",
    "\n",
    "f_raw_input = open(data_path, \"r\")\n",
    "data = json.loads(f_raw_input.read())[\"input_data\"][0]\n",
    "data_tensor = torch.reshape(torch.tensor(data),(1, len(data), 1))\n",
    "\n",
    "#  dummy data for data consumer: make the bound approx same as real data\n",
    "dummy_data = np.random.uniform(min(data), max(data), len(data))\n",
    "json.dump({\"input_data\":[dummy_data.tolist()]}, open(dummy_data_path, 'w'))\n",
    "\n",
    "dummy_data_tensor = torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 ))\n",
    "dummy_theory_output = torch.exp(torch.mean(torch.log(dummy_data_tensor)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier/ data consumer side:\n",
    "class verifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(verifier_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = dummy_theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<0.01*(torch.log(self.w)*X.size()[1]), self.w)\n",
    "\n",
    "verifier_define_calculation(verifier_model, verifier_model_path, [dummy_data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generate & Calibrate Setting ====\n",
      "scale:  [0]\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":0,\"param_scale\":0,\"scale_rebase_multiplier\":10,\"lookup_range\":[-26,176],\"logrows\":14,\"num_inner_cols\":1,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":14432,\"total_assignments\":906,\"total_const_size\":0,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,0],\"model_input_scales\":[0],\"module_sizes\":{\"kzg\":[],\"poseidon\":[14432,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":1.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.0.8\",\"num_blinding_factors\":null}\n"
     ]
    }
   ],
   "source": [
    "# prover calculates settings, send to verifier\n",
    "prover_gen_settings([data_path], verifier_model_path, [0], \"resources\", settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== setting up ezkl ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time setup: 1.3956449031829834 seconds\n",
      "=======================================\n",
      "Theory output:  tensor(47.6981)\n",
      "==== Generating Witness ====\n",
      "witness boolean:  1.0\n",
      "witness result 1 : 48.0\n",
      "==== Generating Proof ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proof:  {'instances': [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [16839043444990017283, 4950423971749937263, 15612601816350174191, 3233154036906899140]]], 'proof': '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', 'transcript_type': 'EVM'}\n",
      "Time gen prf: 2.1762497425079346 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here verifier & prover can concurrently call setup since all params are public to get pk. \n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path )\n",
    "\n",
    "print(\"=======================================\")\n",
    "# Prover generates proof\n",
    "theory_output = torch.exp(torch.mean(torch.log(data_tensor)))\n",
    "print(\"Theory output: \", theory_output)\n",
    "class prover_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prover_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<0.01*(torch.log(self.w)*X.size()[1]), self.w)\n",
    "\n",
    "prover_gen_proof(prover_model, [data_path], witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path, pk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_inputs:  1\n",
      "prf instances:  [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [16839043444990017283, 4950423971749937263, 15612601816350174191, 3233154036906899140]]]\n",
      "proof boolean:  1.0\n",
      "proof result 1 : 48.0\n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "# Verifier verifies\n",
    "verifier_verify(proof_path, settings_path, vk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
