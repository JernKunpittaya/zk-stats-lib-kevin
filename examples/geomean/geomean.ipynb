{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==5.0.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 1)) (5.0.8)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 4)) (1.11.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 5)) (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: statistics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 7)) (1.0.3.5)\n",
      "Requirement already satisfied: onnx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (4.45.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from statistics->-r ../../requirements.txt (line 7)) (0.20.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnx->-r ../../requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->-r ../../requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->-r ../../requirements.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "def export_onnx(model, data_tensor_array, model_loc):\n",
    "  circuit = model()\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  # print(device)\n",
    "\n",
    "  circuit.to(device)\n",
    "\n",
    "  # Flips the neural net into inference mode\n",
    "  circuit.eval()\n",
    "  input_names = []\n",
    "  dynamic_axes = {}\n",
    "\n",
    "  data_tensor_tuple = ()\n",
    "  for i in range(len(data_tensor_array)):\n",
    "    data_tensor_tuple += (data_tensor_array[i],)\n",
    "    input_index = \"input\"+str(i+1)\n",
    "    input_names.append(input_index)\n",
    "    dynamic_axes[input_index] = {0 : 'batch_size'}\n",
    "  dynamic_axes[\"output\"] = {0 : 'batch_size'}\n",
    "\n",
    "  # Export the model\n",
    "  torch.onnx.export(circuit,               # model being run\n",
    "                      data_tensor_tuple,                   # model input (or a tuple for multiple inputs)\n",
    "                      model_loc,            # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=11,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = input_names,   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode is either \"accuracy\" or \"resources\"\n",
    "\n",
    "def gen_settings(comb_data_path, onnx_filename, scale, mode, settings_filename):\n",
    "  print(\"==== Generate & Calibrate Setting ====\")\n",
    "  # Set input to be Poseidon Hash, and param of computation graph to be public\n",
    "  # Poseidon is not homomorphic additive, maybe consider Pedersens or Dory commitment.\n",
    "  gip_run_args = ezkl.PyRunArgs()\n",
    "  gip_run_args.input_visibility = \"hashed\"  # matrix and generalized inverse commitments\n",
    "  gip_run_args.output_visibility = \"public\"   # no parameters used\n",
    "  gip_run_args.param_visibility = \"private\" # should be Tensor(True)--> to enforce arbitrary data in w\n",
    "\n",
    " # generate settings\n",
    "  ezkl.gen_settings(onnx_filename, settings_filename, py_run_args=gip_run_args)\n",
    "  if scale ==\"default\":\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode)\n",
    "  else:\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode, scales = scale)\n",
    "\n",
    "  assert os.path.exists(settings_filename)\n",
    "  assert os.path.exists(comb_data_path)\n",
    "  assert os.path.exists(onnx_filename)\n",
    "  f_setting = open(settings_filename, \"r\")\n",
    "  print(\"scale: \", scale)\n",
    "  print(\"setting: \", f_setting.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_init(verifier_model, verifier_model_path, verifier_compiled_model_path, dummy_data_path_array, settings_path, srs_path, pk_path, vk_path, scale, mode):\n",
    "\n",
    "  # load data from dummy_data_path_array into dummy_data_array\n",
    "  dummy_data_tensor_array = []\n",
    "  comb_dummy_data = []\n",
    "  for path in dummy_data_path_array:\n",
    "    dummy_data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    dummy_data_tensor_array.append(torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 )))\n",
    "    comb_dummy_data.append(dummy_data.tolist())\n",
    "  # export onnx file\n",
    "  export_onnx(verifier_model,dummy_data_tensor_array, verifier_model_path)\n",
    "\n",
    "  comb_dummy_data_path = os.path.join('generated/comb_dummy_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_dummy_data), open(comb_dummy_data_path, 'w' ))\n",
    "\n",
    "  # gen + calibrate setting\n",
    "  gen_settings(comb_dummy_data_path, verifier_model_path, scale, mode, settings_path)\n",
    "\n",
    "  # compile circuit\n",
    "  res = ezkl.compile_circuit(verifier_model_path, verifier_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "\n",
    "  # srs path\n",
    "  res = ezkl.get_srs(srs_path, settings_path)\n",
    "\n",
    "  # setupt vk, pk param for use..... prover can use same pk or can init their own!\n",
    "  print(\"==== setting up ezkl ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.setup(\n",
    "        verifier_compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        srs_path)\n",
    "  end_time = time.time()\n",
    "  time_setup = end_time -start_time\n",
    "  print(f\"Time setup: {time_setup} seconds\")\n",
    "\n",
    "  assert res == True\n",
    "  assert os.path.isfile(vk_path)\n",
    "  assert os.path.isfile(pk_path)\n",
    "  assert os.path.isfile(settings_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prover_gen(prover_model, data_path_array, witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path):\n",
    "  # load data from data_path\n",
    "  # data = json.loads(open(data_path, \"r\").read())[\"input_data\"][0]\n",
    "  # data_tensor = torch.reshape(torch.tensor(data), (1, len(data),1 ))\n",
    "\n",
    "\n",
    "  data_tensor_array = []\n",
    "  comb_data = []\n",
    "  for path in data_path_array:\n",
    "    data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    data_tensor_array.append(torch.reshape(torch.tensor(data), (1, len(data),1 )))\n",
    "    comb_data.append(data.tolist())\n",
    "\n",
    "  # export onnx file\n",
    "  export_onnx(prover_model, data_tensor_array, prover_model_path)\n",
    "\n",
    "  comb_data_path = os.path.join('generated/comb_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "  res = ezkl.compile_circuit(prover_model_path, prover_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "  # now generate the witness file\n",
    "  print('==== Generating Witness ====')\n",
    "  witness = ezkl.gen_witness(comb_data_path, prover_compiled_model_path, witness_path)\n",
    "  assert os.path.isfile(witness_path)\n",
    "  # print(witness[\"outputs\"])\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "  print(\"witness boolean: \", ezkl.vecu64_to_float(witness['outputs'][0][0], output_scale[0]))\n",
    "  for i in range(len(witness['outputs'][1])):\n",
    "    print(\"witness result\", i+1,\":\", ezkl.vecu64_to_float(witness['outputs'][1][i], output_scale[1]))\n",
    "\n",
    "  # GENERATE A PROOF\n",
    "  print(\"==== Generating Proof ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.prove(\n",
    "        witness_path,\n",
    "        prover_compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "        srs_path,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "  print(\"proof: \" ,res)\n",
    "  end_time = time.time()\n",
    "  time_gen_prf = end_time -start_time\n",
    "  print(f\"Time gen prf: {time_gen_prf} seconds\")\n",
    "  assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_verify(proof_path, settings_path, vk_path, srs_path):\n",
    "  # enforce boolean statement to be true\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "\n",
    "  proof = json.load(open(proof_path))\n",
    "  num_inputs = len(settings['model_input_scales'])\n",
    "  print(\"num_inputs: \", num_inputs)\n",
    "  proof[\"instances\"][0][num_inputs] = ezkl.float_to_vecu64(1.0, output_scale[0])\n",
    "  json.dump(proof, open(proof_path, 'w'))\n",
    "\n",
    "  print(\"prf instances: \", proof['instances'])\n",
    "\n",
    "  print(\"proof boolean: \", ezkl.vecu64_to_float(proof['instances'][0][num_inputs], output_scale[0]))\n",
    "  for i in range(num_inputs+1, len(proof['instances'][0])):\n",
    "    print(\"proof result\",i-num_inputs,\":\", ezkl.vecu64_to_float(proof['instances'][0][i], output_scale[1]))\n",
    "\n",
    "\n",
    "  res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "  assert res == True\n",
    "  print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "os.makedirs(os.path.dirname('generated/'), exist_ok=True)\n",
    "verifier_model_path = os.path.join('generated/verifier.onnx')\n",
    "prover_model_path = os.path.join('generated/prover.onnx')\n",
    "verifier_compiled_model_path = os.path.join('generated/verifier.compiled')\n",
    "prover_compiled_model_path = os.path.join('generated/prover.compiled')\n",
    "pk_path = os.path.join('generated/test.pk')\n",
    "vk_path = os.path.join('generated/test.vk')\n",
    "proof_path = os.path.join('generated/test.pf')\n",
    "settings_path = os.path.join('generated/settings.json')\n",
    "srs_path = os.path.join('generated/kzg.srs')\n",
    "witness_path = os.path.join('generated/witness.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================  ZK-STATS FLOW ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data.json')\n",
    "dummy_data_path = os.path.join('generated/dummy_data.json')\n",
    "\n",
    "f_raw_input = open(data_path, \"r\")\n",
    "data = json.loads(f_raw_input.read())[\"input_data\"][0]\n",
    "data_tensor = torch.reshape(torch.tensor(data),(1, len(data), 1))\n",
    "\n",
    "#  dummy data for data consumer: make the bound approx same as real data\n",
    "dummy_data = np.random.uniform(min(data), max(data), len(data))\n",
    "json.dump({\"input_data\":[dummy_data.tolist()]}, open(dummy_data_path, 'w'))\n",
    "\n",
    "dummy_data_tensor = torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 ))\n",
    "dummy_theory_output = torch.exp(torch.mean(torch.log(dummy_data_tensor)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(46.2028, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(dummy_theory_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generate & Calibrate Setting ====\n",
      "scale:  [0]\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":0,\"param_scale\":0,\"scale_rebase_multiplier\":10,\"lookup_range\":[-32,174],\"logrows\":14,\"num_inner_cols\":1,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":14432,\"total_assignments\":906,\"total_const_size\":0,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,0],\"model_input_scales\":[0],\"module_sizes\":{\"kzg\":[],\"poseidon\":[14432,[1]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Ln\":{\"scale\":1.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.0.8\",\"num_blinding_factors\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== setting up ezkl ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time setup: 1.361543893814087 seconds\n"
     ]
    }
   ],
   "source": [
    "# Verifier/ data consumer side:\n",
    "class verifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(verifier_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = dummy_theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<0.01*(torch.log(self.w)*X.size()[1]), self.w)\n",
    "\n",
    "verifier_init(verifier_model, verifier_model_path, verifier_compiled_model_path, [dummy_data_path], settings_path, srs_path, pk_path, vk_path,[0], \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theory output:  tensor(47.6981)\n",
      "==== Generating Witness ====\n",
      "witness boolean:  1.0\n",
      "witness result 1 : 48.0\n",
      "==== Generating Proof ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proof:  {'instances': [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [16839043444990017283, 4950423971749937263, 15612601816350174191, 3233154036906899140]]], 'proof': '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', 'transcript_type': 'EVM'}\n",
      "Time gen prf: 2.1201729774475098 seconds\n"
     ]
    }
   ],
   "source": [
    "# Prover/ data owner side\n",
    "theory_output = torch.exp(torch.mean(torch.log(data_tensor)))\n",
    "print(\"theory output: \", theory_output)\n",
    "class prover_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prover_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.w = nn.Parameter(data = theory_output, requires_grad = False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        return (torch.abs((torch.log(self.w)*X.size()[1])-torch.sum(torch.log(X)))<0.01*(torch.log(self.w)*X.size()[1]), self.w)\n",
    "prover_gen(prover_model, [data_path], witness_path, prover_model_path, prover_compiled_model_path, settings_path, proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_inputs:  1\n",
      "prf instances:  [[[10512373747352303962, 11798585516934984832, 13421675179368312123, 2200257403316998104], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [16839043444990017283, 4950423971749937263, 15612601816350174191, 3233154036906899140]]]\n",
      "proof boolean:  1.0\n",
      "proof result 1 : 48.0\n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "# Verifier verifies\n",
    "verifier_verify(proof_path, settings_path, vk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
