{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==5.0.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 1)) (5.0.8)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 4)) (1.11.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 5)) (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: statistics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 7)) (1.0.3.5)\n",
      "Requirement already satisfied: onnx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (10.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (4.45.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: docutils>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from statistics->-r ../../requirements.txt (line 7)) (0.20.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnx->-r ../../requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->-r ../../requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->-r ../../requirements.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "def export_onnx(model, data_tensor_array, model_loc):\n",
    "  circuit = model()\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  # print(device)\n",
    "\n",
    "  circuit.to(device)\n",
    "\n",
    "  # Flips the neural net into inference mode\n",
    "  circuit.eval()\n",
    "  input_names = []\n",
    "  dynamic_axes = {}\n",
    "\n",
    "  data_tensor_tuple = ()\n",
    "  for i in range(len(data_tensor_array)):\n",
    "    data_tensor_tuple += (data_tensor_array[i],)\n",
    "    input_index = \"input\"+str(i+1)\n",
    "    input_names.append(input_index)\n",
    "    dynamic_axes[input_index] = {0 : 'batch_size'}\n",
    "  dynamic_axes[\"output\"] = {0 : 'batch_size'}\n",
    "\n",
    "  # Export the model\n",
    "  torch.onnx.export(circuit,               # model being run\n",
    "                      data_tensor_tuple,                   # model input (or a tuple for multiple inputs)\n",
    "                      model_loc,            # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=11,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = input_names,   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode is either \"accuracy\" or \"resources\"\n",
    "\n",
    "def gen_settings(comb_data_path, onnx_filename, scale, mode, settings_filename):\n",
    "  print(\"==== Generate & Calibrate Setting ====\")\n",
    "  # Set input to be Poseidon Hash, and param of computation graph to be public\n",
    "  # Poseidon is not homomorphic additive, maybe consider Pedersens or Dory commitment.\n",
    "  gip_run_args = ezkl.PyRunArgs()\n",
    "  gip_run_args.input_visibility = \"hashed\"  # matrix and generalized inverse commitments\n",
    "  gip_run_args.output_visibility = \"public\"   # no parameters used\n",
    "  gip_run_args.param_visibility = \"private\" # should be Tensor(True)--> to enforce arbitrary data in w\n",
    "\n",
    " # generate settings\n",
    "  ezkl.gen_settings(onnx_filename, settings_filename, py_run_args=gip_run_args)\n",
    "  if scale ==\"default\":\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode)\n",
    "  else:\n",
    "    ezkl.calibrate_settings(\n",
    "    comb_data_path, onnx_filename, settings_filename, mode, scales = scale)\n",
    "\n",
    "  assert os.path.exists(settings_filename)\n",
    "  assert os.path.exists(comb_data_path)\n",
    "  assert os.path.exists(onnx_filename)\n",
    "  f_setting = open(settings_filename, \"r\")\n",
    "  print(\"scale: \", scale)\n",
    "  print(\"setting: \", f_setting.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_define_calculation(verifier_model, verifier_model_path, dummy_data_path_array):\n",
    "  # load data from dummy_data_path_array into dummy_data_array\n",
    "  dummy_data_tensor_array = []\n",
    "  # comb_dummy_data = []\n",
    "  for path in dummy_data_path_array:\n",
    "    dummy_data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    # print(\"dumm: \", dummy_data)\n",
    "    dummy_data_tensor_array.append(torch.reshape(torch.tensor(dummy_data), (1, len(dummy_data),1 )))\n",
    "    # comb_dummy_data.append(dummy_data.tolist())\n",
    "  # export onnx file\n",
    "  export_onnx(verifier_model,dummy_data_tensor_array, verifier_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we decide to not have comb_data_path as parameter since a bit redundant parameter.\n",
    "def prover_gen_settings(data_path_array, prover_model,prover_model_path, scale, mode, settings_path):\n",
    "    data_tensor_array=[]\n",
    "    comb_data = []\n",
    "    for path in data_path_array:\n",
    "        data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "        data_tensor_array.append(torch.reshape(torch.tensor(data), (1, len(data),1 )))\n",
    "        comb_data.append(data.tolist())\n",
    "    # this is private to prover since it contains actual data\n",
    "    comb_data_path = os.path.join('prover/comb_data.json')\n",
    "    # Serialize data into file:\n",
    "    json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "    # export onnx file\n",
    "    export_onnx(prover_model, data_tensor_array, prover_model_path)\n",
    "    # gen + calibrate setting\n",
    "    gen_settings(comb_data_path, prover_model_path, scale, mode, settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here prover can concurrently call this since all params are public to get pk. \n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "def verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path ):\n",
    "  # compile circuit\n",
    "  res = ezkl.compile_circuit(verifier_model_path, verifier_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "\n",
    "  # srs path\n",
    "  res = ezkl.get_srs(srs_path, settings_path)\n",
    "\n",
    "  # setupt vk, pk param for use..... prover can use same pk or can init their own!\n",
    "  print(\"==== setting up ezkl ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.setup(\n",
    "        verifier_compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        srs_path)\n",
    "  end_time = time.time()\n",
    "  time_setup = end_time -start_time\n",
    "  print(f\"Time setup: {time_setup} seconds\")\n",
    "\n",
    "  assert res == True\n",
    "  assert os.path.isfile(vk_path)\n",
    "  assert os.path.isfile(pk_path)\n",
    "  assert os.path.isfile(settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prover_gen_proof(prover_model_path, data_path_array, witness_path, prover_compiled_model_path, settings_path, proof_path, pk_path, srs_path):\n",
    "  comb_data = []\n",
    "  for path in data_path_array:\n",
    "    data = np.array(json.loads(open(path, \"r\").read())[\"input_data\"][0])\n",
    "    comb_data.append(data.tolist())\n",
    "\n",
    "  # not necessarily be the same path as in prover_gen_settings, but to make it not confused, make it the same\n",
    "  comb_data_path = os.path.join('shared/comb_data.json')\n",
    "  # Serialize data into file:\n",
    "  json.dump(dict(input_data = comb_data), open(comb_data_path, 'w' ))\n",
    "\n",
    "  res = ezkl.compile_circuit(prover_model_path, prover_compiled_model_path, settings_path)\n",
    "  assert res == True\n",
    "  # now generate the witness file\n",
    "  print('==== Generating Witness ====')\n",
    "  witness = ezkl.gen_witness(comb_data_path, prover_compiled_model_path, witness_path)\n",
    "  assert os.path.isfile(witness_path)\n",
    "  # print(witness[\"outputs\"])\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "  print(\"witness boolean: \", ezkl.vecu64_to_float(witness['outputs'][0][0], output_scale[0]))\n",
    "  for i in range(len(witness['outputs'][1])):\n",
    "    print(\"witness result\", i+1,\":\", ezkl.vecu64_to_float(witness['outputs'][1][i], output_scale[1]))\n",
    "\n",
    "  # GENERATE A PROOF\n",
    "  print(\"==== Generating Proof ====\")\n",
    "  start_time = time.time()\n",
    "  res = ezkl.prove(\n",
    "        witness_path,\n",
    "        prover_compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "        srs_path,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "  print(\"proof: \" ,res)\n",
    "  end_time = time.time()\n",
    "  time_gen_prf = end_time -start_time\n",
    "  print(f\"Time gen prf: {time_gen_prf} seconds\")\n",
    "  assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_verify(proof_path, settings_path, vk_path, srs_path):\n",
    "  # enforce boolean statement to be true\n",
    "  settings = json.load(open(settings_path))\n",
    "  output_scale = settings['model_output_scales']\n",
    "\n",
    "  proof = json.load(open(proof_path))\n",
    "  num_inputs = len(settings['model_input_scales'])\n",
    "  print(\"num_inputs: \", num_inputs)\n",
    "  proof[\"instances\"][0][num_inputs] = ezkl.float_to_vecu64(1.0, output_scale[0])\n",
    "  json.dump(proof, open(proof_path, 'w'))\n",
    "\n",
    "  print(\"prf instances: \", proof['instances'])\n",
    "\n",
    "  print(\"proof boolean: \", ezkl.vecu64_to_float(proof['instances'][0][num_inputs], output_scale[0]))\n",
    "  for i in range(num_inputs+1, len(proof['instances'][0])):\n",
    "    print(\"proof result\",i-num_inputs,\":\", ezkl.vecu64_to_float(proof['instances'][0][i], output_scale[1]))\n",
    "\n",
    "\n",
    "  res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "  assert res == True\n",
    "  print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "os.makedirs(os.path.dirname('shared/'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname('prover/'), exist_ok=True)\n",
    "verifier_model_path = os.path.join('shared/verifier.onnx')\n",
    "prover_model_path = os.path.join('prover/prover.onnx')\n",
    "verifier_compiled_model_path = os.path.join('shared/verifier.compiled')\n",
    "prover_compiled_model_path = os.path.join('prover/prover.compiled')\n",
    "pk_path = os.path.join('shared/test.pk')\n",
    "vk_path = os.path.join('shared/test.vk')\n",
    "proof_path = os.path.join('shared/test.pf')\n",
    "settings_path = os.path.join('shared/settings.json')\n",
    "srs_path = os.path.join('shared/kzg.srs')\n",
    "witness_path = os.path.join('prover/witness.json')\n",
    "# comb_data_path = os.path.join('prover/comb_data.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================  ZK-STATS FLOW ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cov:  3214.750736446082\n",
      "x mean:  49.5\n",
      "y mean:  227.70285322096493\n"
     ]
    }
   ],
   "source": [
    "x_vals_path = os.path.join('x_vals.json')\n",
    "dummy_x_vals_path = os.path.join('shared/dummy_x_vals.json')\n",
    "x_open = open(x_vals_path, \"r\")\n",
    "x_vals= json.loads(x_open.read())['input_data'][0]\n",
    "dummy_x_vals = np.random.uniform(min(x_vals), max(x_vals), len(x_vals))\n",
    "json.dump({\"input_data\":[dummy_x_vals.tolist()]}, open(dummy_x_vals_path, 'w'))\n",
    "\n",
    "\n",
    "y_vals_path = os.path.join('y_vals.json')\n",
    "dummy_y_vals_path = os.path.join('shared/dummy_y_vals.json')\n",
    "y_open = open(y_vals_path, \"r\")\n",
    "y_vals= json.loads(y_open.read())[\"input_data\"][0]\n",
    "dummy_y_vals = np.random.uniform(min(y_vals), max(y_vals), len(y_vals))\n",
    "json.dump({\"input_data\":[dummy_y_vals.tolist()]}, open(dummy_y_vals_path, 'w'))\n",
    "\n",
    "\n",
    "real_cov = statistics.covariance(x_vals, y_vals)\n",
    "x_mean = statistics.mean(x_vals)\n",
    "y_mean = statistics.mean(y_vals)\n",
    "print(\"cov: \",real_cov )\n",
    "print(\"x mean: \", x_mean)\n",
    "print(\"y mean: \", y_mean)\n",
    "\n",
    "dummy_cov = statistics.covariance(dummy_x_vals, dummy_y_vals)\n",
    "dummy_x_mean = statistics.mean(dummy_x_vals)\n",
    "dummy_y_mean = statistics.mean(dummy_y_vals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:2174: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Bool' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
      "  return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))\n"
     ]
    }
   ],
   "source": [
    "# Verifier/ data consumer side:\n",
    "class verifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(verifier_model, self).__init__()\n",
    "        self.cov = nn.Parameter(data = torch.tensor(dummy_cov), requires_grad = False)\n",
    "        self.x_mean = nn.Parameter(data = torch.tensor(dummy_x_mean), requires_grad = False)\n",
    "        self.y_mean = nn.Parameter(data = torch.tensor(dummy_y_mean), requires_grad = False)\n",
    "    def forward(self,X,Y):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        # print(\"x size: \", X.size()[1])\n",
    "        #  need to enforce same length, not yet\n",
    "        x_mean_cons = torch.abs(torch.sum(X)-X.size()[1]*(self.x_mean))<0.01*torch.sum(X)\n",
    "        y_mean_cons = torch.abs(torch.sum(Y)-Y.size()[1]*(self.y_mean))<0.01*torch.sum(Y)\n",
    "        return (torch.logical_and(torch.logical_and(x_mean_cons,y_mean_cons), torch.abs(torch.sum((X-self.x_mean)*(Y-self.y_mean))-(X.size()[1]-1)*(self.cov))<0.01*(X.size()[1]-1)*(self.cov)), self.cov)\n",
    "\n",
    "verifier_define_calculation(verifier_model, verifier_model_path, [dummy_x_vals_path, dummy_y_vals_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theory output:  tensor(3214.7507)\n",
      "==== Generate & Calibrate Setting ====\n",
      "scale:  [0]\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":0,\"param_scale\":0,\"scale_rebase_multiplier\":10,\"lookup_range\":[0,58362],\"logrows\":16,\"num_inner_cols\":1,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":13120,\"total_assignments\":1024,\"total_const_size\":2,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,0],\"model_input_scales\":[0,0],\"module_sizes\":{\"kzg\":[],\"poseidon\":[13120,[2]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"Div\":{\"denom\":100.0}},{\"GreaterThan\":{\"a\":0.0}}],\"check_mode\":\"UNSAFE\",\"version\":\"5.0.8\",\"num_blinding_factors\":null}\n"
     ]
    }
   ],
   "source": [
    "# prover calculates settings, send to verifier\n",
    "\n",
    "theory_output = torch.tensor(real_cov)\n",
    "print(\"Theory output: \", theory_output)\n",
    "class prover_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prover_model, self).__init__()\n",
    "        self.cov = nn.Parameter(data = torch.tensor(real_cov), requires_grad = False)\n",
    "        self.x_mean = nn.Parameter(data = torch.tensor(x_mean), requires_grad = False)\n",
    "        self.y_mean = nn.Parameter(data = torch.tensor(y_mean), requires_grad = False)\n",
    "    def forward(self,X,Y):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        # print(\"x size: \", X.size()[1])\n",
    "        #  need to enforce same length\n",
    "        x_mean_cons = torch.abs(torch.sum(X)-X.size()[1]*(self.x_mean))<0.01*torch.sum(X)\n",
    "        y_mean_cons = torch.abs(torch.sum(Y)-Y.size()[1]*(self.y_mean))<0.01*torch.sum(Y)\n",
    "        return (torch.logical_and(torch.logical_and(x_mean_cons,y_mean_cons), torch.abs(torch.sum((X-self.x_mean)*(Y-self.y_mean))-(X.size()[1]-1)*(self.cov))<0.01*(X.size()[1]-1)*(self.cov)), self.cov)\n",
    "\n",
    "prover_gen_settings([x_vals_path, y_vals_path], prover_model,prover_model_path, [0], \"resources\", settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== setting up ezkl ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time setup: 5.338188171386719 seconds\n",
      "=======================================\n",
      "Theory output:  tensor(3214.7507)\n",
      "==== Generating Witness ====\n",
      "witness boolean:  1.0\n",
      "witness result 1 : 3215.0\n",
      "==== Generating Proof ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proof:  {'instances': [[[14955570959218682635, 4667139652385906200, 12836539004462631467, 1774684518626433649], [2636517083442646612, 5059672846326347070, 4705835036930416229, 3312472395716484608], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [5743885005642251665, 3430503978676436355, 7149667244725939006, 2902673458086333540]]], 'proof': '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', 'transcript_type': 'EVM'}\n",
      "Time gen prf: 6.458252191543579 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here verifier & prover can concurrently call setup since all params are public to get pk. \n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path )\n",
    "\n",
    "print(\"=======================================\")\n",
    "# Prover generates proof\n",
    "print(\"Theory output: \", theory_output)\n",
    "prover_gen_proof(prover_model_path, [x_vals_path, y_vals_path], witness_path, prover_compiled_model_path, settings_path, proof_path, pk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_inputs:  2\n",
      "prf instances:  [[[14955570959218682635, 4667139652385906200, 12836539004462631467, 1774684518626433649], [2636517083442646612, 5059672846326347070, 4705835036930416229, 3312472395716484608], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [5743885005642251665, 3430503978676436355, 7149667244725939006, 2902673458086333540]]]\n",
      "proof boolean:  1.0\n",
      "proof result 1 : 3215.0\n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "# Verifier verifies\n",
    "verifier_verify(proof_path, settings_path, vk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
