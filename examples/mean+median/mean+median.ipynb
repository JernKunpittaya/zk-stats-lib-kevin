{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==7.0.0 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from -r ../../requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: torch in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from -r ../../requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: requests in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from -r ../../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from -r ../../requirements.txt (line 4)) (1.12.0)\n",
      "Requirement already satisfied: numpy in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from -r ../../requirements.txt (line 5)) (1.26.3)\n",
      "Requirement already satisfied: matplotlib in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from -r ../../requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: statistics in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from -r ../../requirements.txt (line 7)) (1.0.3.5)\n",
      "Requirement already satisfied: onnx in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from -r ../../requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: filelock in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from torch->-r ../../requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from torch->-r ../../requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from torch->-r ../../requirements.txt (line 2)) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from requests->-r ../../requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from requests->-r ../../requirements.txt (line 3)) (2024.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: docutils>=0.3 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from statistics->-r ../../requirements.txt (line 7)) (0.20.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from onnx->-r ../../requirements.txt (line 8)) (4.25.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from jinja2->torch->-r ../../requirements.txt (line 2)) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jernkun/Library/Caches/pypoetry/virtualenvs/zkstats-OJpceffF-py3.11/lib/python3.11/site-packages (from sympy->torch->-r ../../requirements.txt (line 2)) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../zkstats/core.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "os.makedirs(os.path.dirname('shared/'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname('prover/'), exist_ok=True)\n",
    "verifier_model_path = os.path.join('shared/verifier.onnx')\n",
    "prover_model_path = os.path.join('prover/prover.onnx')\n",
    "verifier_compiled_model_path = os.path.join('shared/verifier.compiled')\n",
    "prover_compiled_model_path = os.path.join('prover/prover.compiled')\n",
    "pk_path = os.path.join('shared/test.pk')\n",
    "vk_path = os.path.join('shared/test.vk')\n",
    "proof_path = os.path.join('shared/test.pf')\n",
    "settings_path = os.path.join('shared/settings.json')\n",
    "srs_path = os.path.join('shared/kzg.srs')\n",
    "witness_path = os.path.join('prover/witness.json')\n",
    "# this is private to prover since it contains actual data\n",
    "sel_data_path = os.path.join('prover/sel_data.json')\n",
    "# this is just dummy random value\n",
    "sel_dummy_data_path = os.path.join('shared/sel_dummy_data.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================  ZK-STATS FLOW ======================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a weird example where different col1 and col2 of data.json have different row number. We just want to show that different dimension is possible. In fact, a person can just request median(col_1), and median(col_2). Then just compute mean on his own as well, but here we show that the code is composable enough to do all at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = os.path.join('data.json')\n",
    "dummy_data_path = os.path.join('shared/dummy_data.json')\n",
    "\n",
    "data = json.loads(open(data_path, \"r\").read())\n",
    "data1 = data['col_1']\n",
    "data2 = data['col_2']\n",
    "\n",
    "\n",
    "create_dummy(data_path, dummy_data_path)\n",
    "dummy_data = json.loads(open(dummy_data_path, \"r\").read())\n",
    "dummy_data1 = dummy_data['col_1']\n",
    "dummy_data2 = dummy_data['col_2']\n",
    "\n",
    "\n",
    "dummy_theory_output_median1 = torch.tensor(np.median(dummy_data1))\n",
    "dummy_lower_to_median1 = torch.tensor(np.sort(dummy_data1)[int(len(dummy_data1)/2)-1])\n",
    "dummy_upper_to_median1 = torch.tensor(np.sort(dummy_data1)[int(len(dummy_data1)/2)])\n",
    "\n",
    "dummy_theory_output_median2 = torch.tensor(np.median(dummy_data2))\n",
    "dummy_lower_to_median2 = torch.tensor(np.sort(dummy_data2)[int(len(dummy_data2)/2)-1])\n",
    "dummy_upper_to_median2 = torch.tensor(np.sort(dummy_data2)[int(len(dummy_data2)/2)])\n",
    "\n",
    "dummy_theory_output_mean = torch.mean(torch.tensor([dummy_theory_output_median1, dummy_theory_output_median2]))\n",
    "\n",
    "theory_output_median1 = torch.tensor(np.median(data1))\n",
    "lower_to_median1 = torch.tensor(np.sort(data1)[int(len(data1)/2)-1])\n",
    "upper_to_median1 = torch.tensor(np.sort(data1)[int(len(data1)/2)])\n",
    "\n",
    "theory_output_median2 = torch.tensor(np.median(data2))\n",
    "lower_to_median2 = torch.tensor(np.sort(data2)[int(len(data2)/2)-1])\n",
    "upper_to_median2 = torch.tensor(np.sort(data2)[int(len(data2)/2)])\n",
    "\n",
    "theory_output_mean = torch.mean(torch.tensor([theory_output_median1, theory_output_median2]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(X, median, lower, upper):\n",
    "    # since within 1%, we regard as same value\n",
    "    count_less = torch.sum((X < 0.99*median).double())\n",
    "    count_equal = torch.sum((torch.abs(X-median)<=torch.abs(0.01*median)).double())\n",
    "    len = X.size()[1]\n",
    "    half_len = torch.floor(torch.div(len, 2))\n",
    "\n",
    "    # not support modulo yet\n",
    "    less_cons = count_less<half_len+2*(len/2 - torch.floor(len/2))\n",
    "    more_cons = count_less+count_equal>half_len\n",
    "\n",
    "    # For count_equal == 0\n",
    "    lower_exist = torch.sum((torch.abs(X-lower)<=torch.abs(0.01*lower)).double())>0\n",
    "    lower_cons = torch.sum((X>1.01*lower).double())==half_len\n",
    "    upper_exist = torch.sum((torch.abs(X-upper)<=torch.abs(0.01*upper)).double())>0\n",
    "    upper_cons = torch.sum((X<0.99*upper).double())==half_len\n",
    "    bound = 2*count_less==2*half_len\n",
    "    # 0.02 since 2*0.01\n",
    "    bound_avg = (torch.abs(lower+upper-2*median)<=torch.abs(0.02*median))\n",
    "\n",
    "    median_in_cons = torch.logical_and(less_cons, more_cons)\n",
    "    median_out_cons = torch.logical_and(torch.logical_and(bound, bound_avg), torch.logical_and(torch.logical_and(lower_cons, upper_cons), torch.logical_and(lower_exist, upper_exist)))\n",
    "\n",
    "    return(torch.where(count_equal==0, median_out_cons, median_in_cons), median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(X, mean):\n",
    "    return (torch.abs(torch.sum(X)-X.size()[1]*(mean))<=torch.abs(0.01*X.size()[1]*mean), mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [8]\n",
    "selected_columns = ['col_1', 'col_2']\n",
    "commitment_maps = get_data_commitment_maps(data_path, scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy output:  tensor(14.7750, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_93769/4247284567.py:17: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  bool3, output_mean = mean(torch.tensor([median1, median2]).reshape(1,-1,1), self.mean)\n",
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_93769/4247284567.py:17: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  bool3, output_mean = mean(torch.tensor([median1, median2]).reshape(1,-1,1), self.mean)\n"
     ]
    }
   ],
   "source": [
    "print(\"dummy output: \", dummy_theory_output_mean)\n",
    "# Verifier/ data consumer side: send desired calculation\n",
    "class verifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(verifier_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.median1 = nn.Parameter(data = dummy_theory_output_median1, requires_grad = False)\n",
    "        self.lower1 = nn.Parameter(data = dummy_lower_to_median1, requires_grad = False)\n",
    "        self.upper1 = nn.Parameter(data = dummy_upper_to_median1, requires_grad = False)\n",
    "        self.median2 = nn.Parameter(data = dummy_theory_output_median2, requires_grad = False)\n",
    "        self.lower2 = nn.Parameter(data = dummy_lower_to_median2, requires_grad = False)\n",
    "        self.upper2 = nn.Parameter(data = dummy_upper_to_median2, requires_grad = False)\n",
    "        self.mean = nn.Parameter(data = dummy_theory_output_mean, requires_grad = False)\n",
    "    def forward(self,X1, X2):\n",
    "        bool1, median1 = median(X1, self.median1, self.lower1, self.upper1)\n",
    "        bool2, median2 = median(X2, self.median2, self.lower2, self.upper2)\n",
    "        bool3, output_mean = mean(torch.tensor([median1, median2]).reshape(1,-1,1), self.mean)\n",
    "        return (torch.logical_and(torch.logical_and(bool1, bool2),bool3), output_mean )\n",
    "\n",
    "\n",
    "\n",
    "verifier_define_calculation(dummy_data_path, selected_columns,sel_dummy_data_path,verifier_model, verifier_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theory mean output:  tensor(49.3500, dtype=torch.float64)\n",
      "median 1:  tensor(49.5500, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_93769/2138577108.py:19: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  bool3, output_mean = mean(torch.tensor([median1, median2]).reshape(1,-1,1), self.mean)\n",
      "/var/folders/89/y9dw12v976ngdmqz4l7wbsnr0000gn/T/ipykernel_93769/2138577108.py:19: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  bool3, output_mean = mean(torch.tensor([median1, median2]).reshape(1,-1,1), self.mean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generate & Calibrate Setting ====\n",
      "scale:  [8]\n",
      "setting:  {\"run_args\":{\"tolerance\":{\"val\":0.0,\"scale\":1.0},\"input_scale\":8,\"param_scale\":8,\"scale_rebase_multiplier\":10,\"lookup_range\":[-25518,25754],\"logrows\":16,\"num_inner_cols\":2,\"variables\":[[\"batch_size\",1]],\"input_visibility\":{\"Hashed\":{\"hash_is_public\":true,\"outlets\":[]}},\"output_visibility\":\"Public\",\"param_visibility\":\"Private\"},\"num_rows\":20992,\"total_assignments\":16104,\"total_const_size\":2430,\"model_instance_shapes\":[[1],[1]],\"model_output_scales\":[0,8],\"model_input_scales\":[8,8],\"module_sizes\":{\"kzg\":[],\"poseidon\":[20992,[2]],\"elgamal\":[0,[0]]},\"required_lookups\":[\"Abs\",{\"GreaterThan\":{\"a\":0.0}},\"KroneckerDelta\"],\"check_mode\":\"UNSAFE\",\"version\":\"7.0.0\",\"num_blinding_factors\":null}\n"
     ]
    }
   ],
   "source": [
    "# prover calculates settings, send to verifier\n",
    "print(\"theory mean output: \", theory_output_mean)\n",
    "print(\"median 1: \", theory_output_median1)\n",
    "\n",
    "class prover_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prover_model, self).__init__()\n",
    "        # w represents mean in this case\n",
    "        self.median1 = nn.Parameter(data = theory_output_median1, requires_grad = False)\n",
    "        self.lower1 = nn.Parameter(data = lower_to_median1, requires_grad = False)\n",
    "        self.upper1 = nn.Parameter(data = upper_to_median1, requires_grad = False)\n",
    "        self.median2 = nn.Parameter(data = theory_output_median2, requires_grad = False)\n",
    "        self.lower2 = nn.Parameter(data = lower_to_median2, requires_grad = False)\n",
    "        self.upper2 = nn.Parameter(data = upper_to_median2, requires_grad = False)\n",
    "        self.mean = nn.Parameter(data = theory_output_mean, requires_grad = False)\n",
    "    def forward(self,X1, X2):\n",
    "        bool1, median1 = median(X1, self.median1, self.lower1, self.upper1)\n",
    "        bool2, median2 = median(X2, self.median2, self.lower2, self.upper2)\n",
    "        bool3, output_mean = mean(torch.tensor([median1, median2]).reshape(1,-1,1), self.mean)\n",
    "        return (torch.logical_and(torch.logical_and(bool1, bool2),bool3), output_mean )\n",
    "\n",
    "\n",
    "\n",
    "prover_gen_settings(data_path,selected_columns, sel_data_path, prover_model,prover_model_path, scales, \"resources\", settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n",
      "spawning module 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== setting up ezkl ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time setup: 7.375061988830566 seconds\n",
      "=======================================\n",
      "Theory output:  tensor(49.3500, dtype=torch.float64)\n",
      "==== Generating Witness ====\n",
      "witness boolean:  1.0\n",
      "witness result 1 : 49.3515625\n",
      "==== Generating Proof ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proof:  {'instances': [[[3042937791208075219, 8157070662846698822, 3804781648660056856, 172406108020799675], [15295097400487804665, 12861486368330479023, 3350118022201779210, 343142782800691716], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [10870267098303494893, 1752989342377741058, 8860763459400202009, 2635465469930673149]]], 'proof': '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', 'transcript_type': 'EVM'}\n",
      "Time gen prf: 9.826660871505737 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here verifier & prover can concurrently call setup since all params are public to get pk.\n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "setup(verifier_model_path, verifier_compiled_model_path, settings_path,vk_path, pk_path )\n",
    "\n",
    "print(\"=======================================\")\n",
    "# Prover generates proof\n",
    "print(\"Theory output: \", theory_output_mean)\n",
    "prover_gen_proof(prover_model_path, sel_data_path, witness_path, prover_compiled_model_path, settings_path, proof_path, pk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.3515625"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifier verifies\n",
    "verifier_verify(proof_path, settings_path, vk_path, selected_columns, commitment_maps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
