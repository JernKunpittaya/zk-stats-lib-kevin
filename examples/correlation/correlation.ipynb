{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==5.0.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 1)) (5.0.8)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 4)) (1.11.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 5)) (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: statistics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 7)) (1.0.3.5)\n",
      "Requirement already satisfied: onnx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from -r ../../requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->-r ../../requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->-r ../../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (10.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (4.45.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->-r ../../requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: docutils>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from statistics->-r ../../requirements.txt (line 7)) (0.20.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnx->-r ../../requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jernkun/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->-r ../../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->-r ../../requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->-r ../../requirements.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "os.makedirs(os.path.dirname('shared/'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname('prover/'), exist_ok=True)\n",
    "verifier_model_path = os.path.join('shared/verifier.onnx')\n",
    "prover_model_path = os.path.join('prover/prover.onnx')\n",
    "verifier_compiled_model_path = os.path.join('shared/verifier.compiled')\n",
    "prover_compiled_model_path = os.path.join('prover/prover.compiled')\n",
    "pk_path = os.path.join('shared/test.pk')\n",
    "vk_path = os.path.join('shared/test.vk')\n",
    "proof_path = os.path.join('shared/test.pf')\n",
    "settings_path = os.path.join('shared/settings.json')\n",
    "srs_path = os.path.join('shared/kzg.srs')\n",
    "witness_path = os.path.join('prover/witness.json')\n",
    "# this is private to prover since it contains actual data\n",
    "comb_data_path = os.path.join('prover/comb_data.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================  ZK-STATS FLOW ======================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the circuit is too huge, possibly by requesting covariance and stdev and just calculate it on verifier side instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr:  0.8928769029553791\n",
      "x mean:  49.5\n",
      "y mean:  227.703\n",
      "dummy corr:  0.09636370915922249\n",
      "dummy x mean:  49.00761737545071\n",
      "dummy y mean:  237.99976726082184\n"
     ]
    }
   ],
   "source": [
    "x_vals_path = os.path.join('x_vals.json')\n",
    "dummy_x_vals_path = os.path.join('shared/dummy_x_vals.json')\n",
    "x_open = open(x_vals_path, \"r\")\n",
    "x_vals= json.loads(x_open.read())['input_data'][0]\n",
    "dummy_x_vals = np.random.uniform(min(x_vals), max(x_vals), len(x_vals))\n",
    "json.dump({\"input_data\":[dummy_x_vals.tolist()]}, open(dummy_x_vals_path, 'w'))\n",
    "\n",
    "\n",
    "y_vals_path = os.path.join('y_vals.json')\n",
    "dummy_y_vals_path = os.path.join('shared/dummy_y_vals.json')\n",
    "y_open = open(y_vals_path, \"r\")\n",
    "y_vals= json.loads(y_open.read())[\"input_data\"][0]\n",
    "dummy_y_vals = np.random.uniform(min(y_vals), max(y_vals), len(y_vals))\n",
    "json.dump({\"input_data\":[dummy_y_vals.tolist()]}, open(dummy_y_vals_path, 'w'))\n",
    "\n",
    "\n",
    "real_corr = statistics.correlation(x_vals, y_vals)\n",
    "x_mean = statistics.mean(x_vals)\n",
    "y_mean = statistics.mean(y_vals)\n",
    "x_var = np.sum((np.array(x_vals)-x_mean)**2)\n",
    "y_var = np.sum((np.array(y_vals)-y_mean)**2)\n",
    "print(\"corr: \",real_corr )\n",
    "print(\"x mean: \", x_mean)\n",
    "print(\"y mean: \", y_mean)\n",
    "\n",
    "dummy_corr = statistics.correlation(dummy_x_vals, dummy_y_vals)\n",
    "dummy_x_mean = statistics.mean(dummy_x_vals)\n",
    "dummy_y_mean = statistics.mean(dummy_y_vals)\n",
    "dummy_x_var = np.sum((dummy_x_vals-dummy_x_mean)**2)\n",
    "dummy_y_var = np.sum((dummy_y_vals-dummy_y_mean)**2)\n",
    "print(\"dummy corr: \",dummy_corr )\n",
    "print(\"dummy x mean: \", dummy_x_mean)\n",
    "print(\"dummy y mean: \", dummy_y_mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxx:  tensor([[[62.9795],\n",
      "         [22.0217],\n",
      "         [52.3409],\n",
      "         [37.5802],\n",
      "         [58.1255],\n",
      "         [52.4249],\n",
      "         [20.3958],\n",
      "         [47.2491],\n",
      "         [23.8496],\n",
      "         [ 7.8126],\n",
      "         [13.9089],\n",
      "         [79.6785],\n",
      "         [43.2629],\n",
      "         [62.6596],\n",
      "         [92.1490],\n",
      "         [17.4866],\n",
      "         [14.0891],\n",
      "         [ 4.9190],\n",
      "         [78.6948],\n",
      "         [41.2017],\n",
      "         [42.2647],\n",
      "         [43.9585],\n",
      "         [83.9661],\n",
      "         [89.5124],\n",
      "         [95.3961],\n",
      "         [60.9797],\n",
      "         [23.4067],\n",
      "         [17.3428],\n",
      "         [90.6992],\n",
      "         [79.7309],\n",
      "         [87.4945],\n",
      "         [45.8548],\n",
      "         [22.1585],\n",
      "         [92.6558],\n",
      "         [ 7.5434],\n",
      "         [ 6.0029],\n",
      "         [46.7445],\n",
      "         [59.2296],\n",
      "         [26.6279],\n",
      "         [43.9125],\n",
      "         [ 2.2883],\n",
      "         [52.6816],\n",
      "         [35.7346],\n",
      "         [54.6388],\n",
      "         [75.4577],\n",
      "         [ 7.3459],\n",
      "         [82.0465],\n",
      "         [85.7411],\n",
      "         [46.4742],\n",
      "         [64.1138],\n",
      "         [46.7118],\n",
      "         [85.1890],\n",
      "         [44.0479],\n",
      "         [86.8898],\n",
      "         [19.1891],\n",
      "         [38.0513],\n",
      "         [73.5213],\n",
      "         [77.7149],\n",
      "         [68.4295],\n",
      "         [37.9432],\n",
      "         [83.8206],\n",
      "         [65.2856],\n",
      "         [91.2397],\n",
      "         [70.4041],\n",
      "         [67.8913],\n",
      "         [16.5063],\n",
      "         [64.8318],\n",
      "         [28.4944],\n",
      "         [29.0290],\n",
      "         [62.3993],\n",
      "         [19.7002],\n",
      "         [63.5303],\n",
      "         [48.8265],\n",
      "         [36.1527],\n",
      "         [82.4579],\n",
      "         [93.9550],\n",
      "         [71.1069],\n",
      "         [52.8947],\n",
      "         [32.2252],\n",
      "         [38.8979],\n",
      "         [35.0119],\n",
      "         [28.1229],\n",
      "         [45.7980],\n",
      "         [58.1556],\n",
      "         [63.2104],\n",
      "         [10.0951],\n",
      "         [22.9158],\n",
      "         [79.7788],\n",
      "         [ 4.3663],\n",
      "         [48.3876],\n",
      "         [94.3303],\n",
      "         [56.7153],\n",
      "         [83.0030],\n",
      "         [10.8275],\n",
      "         [13.0768],\n",
      "         [19.4223],\n",
      "         [ 1.9992],\n",
      "         [36.1875],\n",
      "         [ 5.3506],\n",
      "         [81.8356]]], dtype=torch.float64)\n",
      "mean X:  tensor(49.0076, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:2174: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Bool' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
      "  return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))\n"
     ]
    }
   ],
   "source": [
    "# precise float number is hard, so we calculate 100*correlation instead.\n",
    "# Verifier/ data consumer side:\n",
    "class verifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(verifier_model, self).__init__()\n",
    "        self.corr = nn.Parameter(data = torch.tensor(dummy_corr*100), requires_grad = False)\n",
    "        self.x_mean = nn.Parameter(data = torch.tensor(dummy_x_mean), requires_grad = False)\n",
    "        self.y_mean = nn.Parameter(data = torch.tensor(dummy_y_mean), requires_grad = False)\n",
    "        self.x_var = nn.Parameter(data = torch.tensor(dummy_x_var), requires_grad = False)\n",
    "        self.y_var = nn.Parameter(data = torch.tensor(dummy_y_var), requires_grad = False)\n",
    "    def forward(self,X,Y):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        #  need to enforce same length, not yet\n",
    "        x_mean_cons = torch.abs(torch.sum(X)-X.size()[1]*(self.x_mean))<0.01*X.size()[1]*(self.x_mean)\n",
    "        y_mean_cons = torch.abs(torch.sum(Y)-Y.size()[1]*(self.y_mean))<0.01*Y.size()[1]*(self.y_mean)\n",
    "        x_var_cons = torch.abs(torch.sum((X-self.x_mean)*(X-self.x_mean))-self.x_var*(X.size()[1]-1))<0.01*self.x_var*(X.size()[1]-1)\n",
    "        y_var_cons = torch.abs(torch.sum((Y-self.y_mean)*(Y-self.y_mean))-self.y_var*(Y.size()[1]-1))<0.01*self.y_var*(Y.size()[1]-1)\n",
    "        corr_cons = torch.abs(torch.sum((X-self.x_mean)*(Y-self.y_mean))-(torch.sqrt(self.x_var)*torch.sqrt(self.y_var)*torch.div(self.corr, 100)))<0.01*(torch.sqrt(self.x_var)*torch.sqrt(self.y_var)*torch.div(self.corr, 100))\n",
    "\n",
    "        # return (torch.logical_and(corr_cons, torch.logical_and(torch.logical_and(x_mean_cons, y_mean_cons), torch.logical_and(x_var_cons, y_var_cons))), self.corr)\n",
    "        # return (torch.logical_and(corr_cons, torch.logical_and(x_mean_cons, y_mean_cons)), self.corr)\n",
    "        # return (torch.logical_and(torch.logical_and(x_var_cons,corr_cons), torch.logical_and(x_mean_cons, y_mean_cons)), self.corr)\n",
    "        # return (torch.logical_and(x_var_cons,torch.logical_and(corr_cons, torch.logical_and(x_mean_cons, y_mean_cons))), self.corr)\n",
    "        return (torch.logical_and(x_var_cons, corr_cons), self.corr)\n",
    "verifier_define_calculation(verifier_model, verifier_model_path, [dummy_x_vals_path, dummy_y_vals_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theory output:  tensor(0.8929)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "std and var only support floating point and complex dtypes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[39m# return (torch.logical_and(corr_cons, torch.logical_and(torch.logical_and(x_mean_cons, y_mean_cons), torch.logical_and(x_var_cons, y_var_cons))), self.corr)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \u001b[39m# return (torch.logical_and(corr_cons, torch.logical_and(x_mean_cons, y_mean_cons)), self.corr)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \u001b[39m# return (torch.logical_and(torch.logical_and(corr_cons, x_var_cons), torch.logical_and(x_mean_cons, y_mean_cons)), self.corr)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[39m# return (torch.logical_and(x_var_cons,torch.logical_and(corr_cons, torch.logical_and(x_mean_cons, y_mean_cons))), self.corr)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[39mreturn\u001b[39;00m (torch\u001b[39m.\u001b[39mlogical_and(x_var_cons, corr_cons), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorr)\n\u001b[0;32m---> 27\u001b[0m prover_gen_settings([x_vals_path, y_vals_path], comb_data_path, prover_model,prover_model_path, [\u001b[39m0\u001b[39;49m], \u001b[39m\"\u001b[39;49m\u001b[39mresources\u001b[39;49m\u001b[39m\"\u001b[39;49m, settings_path)\n",
      "File \u001b[0;32m~/Desktop/zk-stats-lib/core.py:97\u001b[0m, in \u001b[0;36mprover_gen_settings\u001b[0;34m(data_path_array, comb_data_path, prover_model, prover_model_path, scale, mode, settings_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m json\u001b[39m.\u001b[39mdump(\u001b[39mdict\u001b[39m(input_data \u001b[39m=\u001b[39m comb_data), \u001b[39mopen\u001b[39m(comb_data_path, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m ))\n\u001b[1;32m     96\u001b[0m \u001b[39m# export onnx file\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m export_onnx(prover_model, data_tensor_array, prover_model_path)\n\u001b[1;32m     98\u001b[0m \u001b[39m# gen + calibrate setting\u001b[39;00m\n\u001b[1;32m     99\u001b[0m gen_settings(comb_data_path, prover_model_path, scale, mode, settings_path)\n",
      "File \u001b[0;32m~/Desktop/zk-stats-lib/core.py:31\u001b[0m, in \u001b[0;36mexport_onnx\u001b[0;34m(model, data_tensor_array, model_loc)\u001b[0m\n\u001b[1;32m     28\u001b[0m dynamic_axes[\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m : \u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m     30\u001b[0m \u001b[39m# Export the model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(circuit,               \u001b[39m# model being run\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m                     data_tensor_tuple,                   \u001b[39m# model input (or a tuple for multiple inputs)\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m                     model_loc,            \u001b[39m# where to save the model (can be a file or file-like object)\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m                     export_params\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,        \u001b[39m# store the trained parameter weights inside the model file\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m                     opset_version\u001b[39m=\u001b[39;49m\u001b[39m11\u001b[39;49m,          \u001b[39m# the ONNX version to export the model to\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m                     do_constant_folding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# whether to execute constant folding for optimization\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m                     input_names \u001b[39m=\u001b[39;49m input_names,   \u001b[39m# the model's input names\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m                     output_names \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m# the model's output names\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m                     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/onnx/utils.py:1596\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1594\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1596\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1597\u001b[0m     model,\n\u001b[1;32m   1598\u001b[0m     args,\n\u001b[1;32m   1599\u001b[0m     verbose,\n\u001b[1;32m   1600\u001b[0m     input_names,\n\u001b[1;32m   1601\u001b[0m     output_names,\n\u001b[1;32m   1602\u001b[0m     operator_export_type,\n\u001b[1;32m   1603\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1604\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1605\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1606\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1607\u001b[0m )\n\u001b[1;32m   1609\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1611\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1612\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/onnx/utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1134\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/onnx/utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m   1012\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1013\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/onnx/utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    914\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    916\u001b[0m     model,\n\u001b[1;32m    917\u001b[0m     args,\n\u001b[1;32m    918\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    919\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    920\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m )\n\u001b[1;32m    922\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/jit/_trace.py:1285\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1284\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1285\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[1;32m   1286\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[1;32m   1287\u001b[0m )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1288\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/jit/_trace.py:133\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 133\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    134\u001b[0m     wrapper,\n\u001b[1;32m    135\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    136\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/jit/_trace.py:124\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    123\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 124\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    126\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[0;32mIn[37], line 18\u001b[0m, in \u001b[0;36mprover_model.forward\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     16\u001b[0m y_mean_cons \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39msum(Y)\u001b[39m-\u001b[39mY\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_mean))\u001b[39m<\u001b[39m\u001b[39m0.01\u001b[39m\u001b[39m*\u001b[39mY\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_mean)\n\u001b[1;32m     17\u001b[0m x_var_cons \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39msum((X\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_mean)\u001b[39m*\u001b[39m(X\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_mean))\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_var\u001b[39m*\u001b[39m(X\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m<\u001b[39m\u001b[39m0.01\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_var\u001b[39m*\u001b[39m(X\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m x_var \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mvar(X, correction\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m y_var_cons \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39msum((Y\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_mean)\u001b[39m*\u001b[39m(Y\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_mean))\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_var\u001b[39m*\u001b[39m(Y\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m<\u001b[39m\u001b[39m0.01\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_var\u001b[39m*\u001b[39m(Y\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m corr_cons \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39msum((X\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_mean)\u001b[39m*\u001b[39m(Y\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_mean))\u001b[39m-\u001b[39m(torch\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_var)\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_var)\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39mdiv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorr, \u001b[39m100\u001b[39m)))\u001b[39m<\u001b[39m\u001b[39m0.01\u001b[39m\u001b[39m*\u001b[39m(torch\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_var)\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_var)\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39mdiv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorr, \u001b[39m100\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: std and var only support floating point and complex dtypes"
     ]
    }
   ],
   "source": [
    "# Prover/ data owner side\n",
    "theory_output = torch.tensor(real_corr)\n",
    "print(\"theory output: \", theory_output)\n",
    "class prover_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(prover_model, self).__init__()\n",
    "        self.corr = nn.Parameter(data = torch.tensor(real_corr*100), requires_grad = False)\n",
    "        self.x_mean = nn.Parameter(data = torch.tensor(x_mean), requires_grad = False)\n",
    "        self.y_mean = nn.Parameter(data = torch.tensor(y_mean), requires_grad = False)\n",
    "        self.x_var = nn.Parameter(data = torch.tensor(x_var), requires_grad = False)\n",
    "        self.y_var = nn.Parameter(data = torch.tensor(y_var), requires_grad = False)\n",
    "    def forward(self,X,Y):\n",
    "        # some expression of tolerance to error in the inference\n",
    "        #  need to enforce same length, not yet\n",
    "        x_mean_cons = torch.abs(torch.sum(X)-X.size()[1]*(self.x_mean))<0.01*X.size()[1]*(self.x_mean)\n",
    "        y_mean_cons = torch.abs(torch.sum(Y)-Y.size()[1]*(self.y_mean))<0.01*Y.size()[1]*(self.y_mean)\n",
    "        x_var_cons = torch.abs(torch.sum((X-self.x_mean)*(X-self.x_mean))-self.x_var*(X.size()[1]-1))<0.01*self.x_var*(X.size()[1]-1)\n",
    "        y_var_cons = torch.abs(torch.sum((Y-self.y_mean)*(Y-self.y_mean))-self.y_var*(Y.size()[1]-1))<0.01*self.y_var*(Y.size()[1]-1)\n",
    "        corr_cons = torch.abs(torch.sum((X-self.x_mean)*(Y-self.y_mean))-(torch.sqrt(self.x_var)*torch.sqrt(self.y_var)*torch.div(self.corr, 100)))<0.01*(torch.sqrt(self.x_var)*torch.sqrt(self.y_var)*torch.div(self.corr, 100))\n",
    "\n",
    "        # return (torch.logical_and(corr_cons, torch.logical_and(torch.logical_and(x_mean_cons, y_mean_cons), torch.logical_and(x_var_cons, y_var_cons))), self.corr)\n",
    "        # return (torch.logical_and(corr_cons, torch.logical_and(x_mean_cons, y_mean_cons)), self.corr)\n",
    "        # return (torch.logical_and(torch.logical_and(corr_cons, x_var_cons), torch.logical_and(x_mean_cons, y_mean_cons)), self.corr)\n",
    "        # return (torch.logical_and(x_var_cons,torch.logical_and(corr_cons, torch.logical_and(x_mean_cons, y_mean_cons))), self.corr)\n",
    "        return (torch.logical_and(x_var_cons, corr_cons), self.corr)\n",
    "prover_gen_settings([x_vals_path, y_vals_path], comb_data_path, prover_model,prover_model_path, [0], \"resources\", settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 0\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== setting up ezkl ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at src/circuit/ops/region.rs:237:38:\n",
      "called `Option::unwrap()` on a `None` value\n"
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "called `Option::unwrap()` on a `None` value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Here verifier & prover can concurrently call setup since all params are public to get pk. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path )\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=======================================\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Prover generates proof\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/zk-stats-lib/core.py:117\u001b[0m, in \u001b[0;36mverifier_setup\u001b[0;34m(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path, vk_path, pk_path)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m==== setting up ezkl ====\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 117\u001b[0m res \u001b[39m=\u001b[39m ezkl\u001b[39m.\u001b[39;49msetup(\n\u001b[1;32m    118\u001b[0m       verifier_compiled_model_path,\n\u001b[1;32m    119\u001b[0m       vk_path,\n\u001b[1;32m    120\u001b[0m       pk_path,\n\u001b[1;32m    121\u001b[0m       srs_path)\n\u001b[1;32m    122\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    123\u001b[0m time_setup \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39mstart_time\n",
      "\u001b[0;31mPanicException\u001b[0m: called `Option::unwrap()` on a `None` value"
     ]
    }
   ],
   "source": [
    "# Here verifier & prover can concurrently call setup since all params are public to get pk. \n",
    "# Here write as verifier function to emphasize that verifier must calculate its own vk to be sure\n",
    "verifier_setup(verifier_model_path, verifier_compiled_model_path, settings_path, srs_path,vk_path, pk_path )\n",
    "\n",
    "print(\"=======================================\")\n",
    "# Prover generates proof\n",
    "print(\"Theory output: \", theory_output)\n",
    "prover_gen_proof(prover_model_path, comb_data_path, witness_path, prover_compiled_model_path, settings_path, proof_path, pk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_inputs:  2\n",
      "prf instances:  [[[14955570959218682635, 4667139652385906200, 12836539004462631467, 1774684518626433649], [4224417983558473805, 851357164555783563, 5363851773531956453, 1448631618362554917], [12436184717236109307, 3962172157175319849, 7381016538464732718, 1011752739694698287], [6856486776492523050, 3509301300289033549, 11286023888431465720, 2871037162753880935]]]\n",
      "proof boolean:  1.0\n",
      "proof result 1 : 89.0\n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "# Verifier verifies\n",
    "verifier_verify(proof_path, settings_path, vk_path, srs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
